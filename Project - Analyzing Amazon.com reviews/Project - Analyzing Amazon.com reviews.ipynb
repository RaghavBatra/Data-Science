{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Analysing Amazon.com reviews\n",
    "\n",
    " **Note**: *This data is based off Amazon's public pages, and the aim of this project is  **NOT** to violate any of Amazon's policies*.  \n",
    "\n",
    "Today I will use web scraping tools, Natural Language Processing, data cleaning and data analysis to find the most common words in Amazon reviews of books. \n",
    "\n",
    "I will use \"Harry Potter and the Deathly Hallows\" as a standing example.\n",
    "\n",
    "Let's start with the libraries that need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make HTTP calls\n",
    "import requests\n",
    "\n",
    "# parse HTML files\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# use NLP\n",
    "import nltk\n",
    "\n",
    "# pretty printing\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one large function that will explained step by step with screenshots and extensive comments throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function signature: takes the book name, author and list of tags (to be explained later)\n",
    "def word_freq(name, author, list_of_tags):\n",
    "\n",
    "    # The goal here is to fully automate extracting reviews. \n",
    "    # To start with, we need the corresponding Amazon page URL that users see, \n",
    "    # where the book's types and prices are listed. \n",
    "    # This page contains the ASIN (Amazon Standard Identification Numbers: https://www.amazon.com/gp/seller/asin-upc-isbn-info.html), \n",
    "    # which will be extremely crucial in getting the reviews.\n",
    "\n",
    "    # Using that, one can identify the URL of the book review's page.\n",
    "    # To that extent, a call to the Google Search API is made. \n",
    "    # According to the official documention, the method to encode the \n",
    "    # search query is 'http://google.com/search?q=[query]+Amazon.com+Book' with\n",
    "    # + as the delimiters between any word in the query.\n",
    "    # Amazon.com and Book are included in the search by default as Amazon.com is\n",
    "    # our target site and 'Book' is mentioned to disambuguate the book from any\n",
    "    # movie of the same name, so that the search results are URLs for the book\n",
    "    # and not the movie.\n",
    "\n",
    "    # For example, to make a search for \"Harry Potter and the Deathly Hallows\" with author \"JK Rowling\", \n",
    "    # the correct encoding is 'http://google.com/search?q=Harry+Potter+and+the+Deathly+Hallows+JK+Rowling+Amazon.com+Book'.\n",
    "\n",
    "    # To that extent, lines 24 - 28 encode the book name and author in the above format.\n",
    "\n",
    "    book_list = name.split()\n",
    "    author_list = author.split()\n",
    "\n",
    "    query = \"+\".join(book_list)\n",
    "    query += \"+\" + \"+\".join(author_list)\n",
    "\n",
    "    # Line 32 puts the query string together and makes a request using the inbuilt requests library.\n",
    "    # The 'dummy' part of the code naming will be explained later.\n",
    "    \n",
    "    dummy_r = requests.get('http://google.com/search?q=' + query + '+Amazon.com+Book')\n",
    "    \n",
    "    # extract the text (HTML) content of the requested page\n",
    "    dummy_source_code = dummy_r.text\n",
    "    \n",
    "    # make this text more accessible by converting it to HTML Document Object Tree (if you don't know what this term means,\n",
    "    # just think of the output of dummy_soup as a tree with the first tag (<html>) as the root, and other nested tags in it\n",
    "    # such as <head> and <body> as its children. The conversion makes accessing the children and particular tags easier).\n",
    "    # Done through the BeautifulSoup library\n",
    "    dummy_soup = BeautifulSoup(dummy_source_code, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the HTML content of the search results page, we need the correct URL to make a request once again. \n",
    "\n",
    "We can't just do with ANY of the search results; we need 'amazon.com'! \n",
    "To find the correct one, we will have to get a list of all the URLs and 'find' the one we want.\n",
    "\n",
    "<img src = \"GS1.png\" style = \"width: 50%;\">\n",
    "<img src = \"GS2.png\" style = \"width: 50%;\">\n",
    "\n",
    "The above images show the results page with our query. \n",
    "We want the Amazon.com one, which has been boxed for convenience.\n",
    "\n",
    "Note that the first few entries are NOT '.com' entries. Google probably suggests local Amazon sites to the global American one. \n",
    "Since I am running this search from India, the '.in' makes sense.\n",
    "\n",
    "\n",
    "But how does one get the URL now that we now where it is? Chrome's Inspector comes to help.\n",
    "\n",
    "After some digging I found that each review 'rectangle' is enclosed within an 'h3' tag.\n",
    "Within that, the URL is the 'href' attribute of the 'a' tag within it.\n",
    "\n",
    "See image below for details.\n",
    "\n",
    "<img src = \"GS3.png\" style = \"width: 50%;\">\n",
    "\n",
    "Time to also use BeautifulSoup again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # purpose explained abovr\n",
    "    h3_lst = dummy_soup.find_all('h3')\n",
    "    list_of_urls = []\n",
    "    \n",
    "    # loop for all h3 elements\n",
    "    for items in h3_lst:\n",
    "        try:\n",
    "            a = items.find('a')\n",
    "            href = a['href']\n",
    "\n",
    "            # only want the main Amazon site. Note the '/' at the end is important.\n",
    "            # Without it, sites such as amazon.com.au will also match.\n",
    "            # A more sophisticated way would be to use regex, but this does the trick!\n",
    "            if 'amazon.com/' in href: \n",
    "                \n",
    "                # remove extraneous URL information that is sometimes found in certain URLs\n",
    "                start = href.index(\"=\")\n",
    "                end = href.index(\"&\")\n",
    "                href_stripped = href[start + 1:end]\n",
    "                \n",
    "                # If you look at the extracted URl, it will be like\n",
    "                # 'https://www.amazon.com/Harry-Potter-Deathly-Hallows-Rowling/dp/1606868829'\n",
    "                # The number after '/dp/' is the ASIN!\n",
    "                \n",
    "                # Turns out to get the main review page, we just need to replace the 'dp' with\n",
    "                # 'product-reviews'. That's it!\n",
    "                \n",
    "                href_new = href_stripped.replace('/dp/', '/product-reviews/')\n",
    "                \n",
    "                list_of_urls.append(href_new)\n",
    "                \n",
    "        except:\n",
    "            # incase of error, do nothing\n",
    "            pass\n",
    "\n",
    "    # Assuming multiple queries, I take the first one.\n",
    "    # This is because Google ranks queries by 'popularity', and presuming\n",
    "    # 'popularity' here translates to accuracy, the first one would be the most\n",
    "    # likely one to take\n",
    "    dummy_url = list_of_urls[0]\n",
    "\n",
    "    # extract text and convert to HTML, as before\n",
    "    amazon = requests.get(dummy_url)\n",
    "    amazon_soup = BeautifulSoup(amazon.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what one should see as the product reviews page for the book\n",
    "\n",
    "<img src = \"review1.png\" style = \"width: 80%;\">\n",
    "\n",
    "If one scrolls down past the 'Top positive review' area, one will notice that there are 10 reviews on the page.\n",
    "\n",
    "While, it would be good to get these reviews, it would be better to get ALL possible reviews for this book.\n",
    "\n",
    "There are 2 factors to consider here:\n",
    "1. How many pages of reviews are there?\n",
    "2. How does one extract the above fact?\n",
    "\n",
    "<img src = \"review2.png\" style = \"width: 80%;\">\n",
    "\n",
    "The first question is solved by scrolling down to the bottom of the page, where one finds the buttons to move through the button. One wants the last page here (boxed). \n",
    "\n",
    "Now for the second question! As done above, snoop around with the Inspector, and you find that the buttons are part of a 'ul' tag, and this element is the last 'li' element in this list. \n",
    "The below code helps achieve exactly that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # find ul tag\n",
    "    ul = amazon_soup.find(\"ul\", class_ = \"a-pagination\")\n",
    "\n",
    "    # find last page number\n",
    "    last_li = ul.find_all('li', class_ = \"page-button\")[-1]\n",
    "    \n",
    "    # convert string value to integer\n",
    "    last_li = int(last_li.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # now that we have the total number of pages, we can loop over that number to get the URLs of all the pages\n",
    "\n",
    "    # store all page URLs in this list\n",
    "    review_pages = []\n",
    "\n",
    "    # Turns out that the pages follow a common format, with only the page number value changing in the URL.\n",
    "    # Look at the below code for reference\n",
    "\n",
    "    for i in range(last_li):\n",
    "        next_page = dummy_url + '/ref=cm_cr_getr_d_paging_btm_' + str(i) + '?pageNumber=' + str(i)\n",
    "        review_pages.append(next_page)\n",
    "        \n",
    "    # Now I explain why I called the earlier HTML page 'dummy'.\n",
    "    # The reason was that that page was only to get the generic product reviews page URL\n",
    "    # Using that, we have the actual page URLs that we need and don't need the earlier variables relating\n",
    "    # to that URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " reviews_list = []\n",
    "    \n",
    "    # [review_pages] now contains the URLs of all the review pages.\n",
    "    # Loop through each, making a request turn-by-turn\n",
    "    for pages in review_pages:\n",
    "\n",
    "        r = requests.get(pages)    \n",
    "        source_code = r.text\n",
    "        soup = BeautifulSoup(source_code, \"lxml\")\n",
    "\n",
    "        # find actual text\n",
    "        # Again, found by fiddling with the Inspector\n",
    "        reviews = soup.find_all(\"div\", class_ = \"a-section review\")\n",
    "\n",
    "        # [reviews] is the list of 10 reviews found on each page.\n",
    "        # Loop through each of them and extract the review text\n",
    "        for review in reviews:\n",
    "            review_text = review.find(\"span\", class_ = \"a-size-base review-text\").text\n",
    "            reviews_list.append(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # [reviews_list] is a list of all review text.\n",
    "    # we will use a dictionary to keep a count of word frequency\n",
    "    dict_tags = {}\n",
    "\n",
    "        for reviews in reviews_list:\n",
    "            \n",
    "            # use NLP to identify each word with its part of speech.\n",
    "            # Refer to http://www.nltk.org/data.html for an example of what the next\n",
    "            # 2 lines will do.\n",
    "            tokens = nltk.word_tokenize(reviews)\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "            for tags in tagged:\n",
    "                # As written below, [tag_text] would be something like 'the'\n",
    "                # and [tag_type] would be 'DT', which stands for determiner,\n",
    "                # as 'the' is a determiner.\n",
    "                \n",
    "                # Refer to http://nishutayaltech.blogspot.in/2015/02/penn-treebank-pos-tags-in-natural.html\n",
    "                # for a list of all types of possible parts of speech (POS) and their abbreviations\n",
    "                tag_text = tags[0]\n",
    "                tag_type = tags[1]\n",
    "\n",
    "                # only consider parts of speech in [list_of_tags]\n",
    "                \n",
    "                # Thus, [list_of_tags] is a list of all possible parts of speeches used as filters.\n",
    "                # This is done because if we just make a dictionary of all possible words in the\n",
    "                # reviews, common words such as 'the' and 'a' will have the highest frequency.\n",
    "                # Now, if that is what you want, they you have the opportunity to put such parts of speech.\n",
    "                # But this function is meant to work with any set of parts of speech the user provides\n",
    "                \n",
    "                if tag_type in list_of_tags:\n",
    "\n",
    "                    # normal mapping of frequencies of words in a dictionary\n",
    "                    if tag_text in dict_tags:\n",
    "                        dict_tags[tag_text] += 1\n",
    "                    else:\n",
    "                        dict_tags[tag_text] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # [dict_sorted] contains a set of words sorted by descending frequency occurence\n",
    "    dict_sorted = sorted(dict_tags.items(), key=lambda x:x[1], reverse = True) # descending\n",
    "\n",
    "    # finding relative frequency of each word in the dictionary\n",
    "    \n",
    "    # sum of frequencies\n",
    "    sum_freq = float(sum(dict_tags.values()))\n",
    "\n",
    "    rel_freq_list = []\n",
    "\n",
    "    # calculate relative frequencies\n",
    "    for tags in dict_sorted:\n",
    "        \n",
    "        # adding all values to a new dictionary with the absolute frequency replaced by the relative frequency\n",
    "        tags_text = ( round(tags[1]/sum_freq, 3) )\n",
    "        rel_freq_list.append( (tags[0], tags_text) )\n",
    "\n",
    "    # [rel_freq_list] is the dictionary of words with relative frequencies\n",
    "    return rel_freq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! We're done!\n",
    "\n",
    "Below is the entire code written in snippets above in one function that takes in the book name, author and the parts of speech worth exploring.\n",
    "\n",
    "Try it out after going through the entire example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def word_freq(name, author, list_of_tags):\n",
    "\n",
    "    # extract book name and author to form search query\n",
    "    book_list = name.split()\n",
    "    author_list = author.split()\n",
    "\n",
    "    query = \"+\".join(book_list)\n",
    "    query += \"+\" + \"+\".join(author_list)\n",
    "\n",
    "    # search the web\n",
    "    dummy_r = requests.get('http://google.com/search?q=' + query + '+Amazon.com+Book')\n",
    "\n",
    "    dummy_source_code = dummy_r.text\n",
    "    # print dummy_source_code\n",
    "\n",
    "    dummy_soup = BeautifulSoup(dummy_source_code, \"lxml\")\n",
    "\n",
    "    h3_lst = dummy_soup.find_all('h3')\n",
    "    list_of_urls = []\n",
    "    \n",
    "    for items in h3_lst:\n",
    "        try:\n",
    "            a = items.find('a')\n",
    "            href = a['href']\n",
    "            if 'amazon.com/' in href: # main Amazon site\n",
    "                start = href.index(\"=\")\n",
    "                end = href.index(\"&\")\n",
    "                href_stripped = href[start + 1:end]\n",
    "                href_new = href_stripped.replace('/dp/', '/product-reviews/')\n",
    "                list_of_urls.append(href_new)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    # use first search query\n",
    "    dummy_url = list_of_urls[0]\n",
    "\n",
    "    amazon = requests.get(dummy_url)\n",
    "    amazon_soup = BeautifulSoup(amazon.text, \"lxml\")\n",
    "\n",
    "    # find number of pages of reviews\n",
    "    ul = amazon_soup.find(\"ul\", class_ = \"a-pagination\")\n",
    "\n",
    "    # find last page number\n",
    "    last_li = ul.find_all('li', class_ = \"page-button\")[-1]\n",
    "    last_li = int(last_li.text)\n",
    "\n",
    "    # store all page URLs in this list\n",
    "    review_pages = []\n",
    "\n",
    "    for i in range(last_li):\n",
    "        next_page = dummy_url + '/ref=cm_cr_getr_d_paging_btm_' + str(i) + '?pageNumber=' + str(i)\n",
    "        review_pages.append(next_page)\n",
    "\n",
    "    # print \"Review pages: \"\n",
    "    # print review_pages\n",
    "\n",
    "    reviews_list = []\n",
    "\n",
    "    for pages in review_pages:\n",
    "        # print \"Page: \"\n",
    "        # print page\n",
    "        # make a request to all pages turn-by-turn\n",
    "        r = requests.get(pages)    \n",
    "        source_code = r.text\n",
    "        soup = BeautifulSoup(source_code, \"lxml\")\n",
    "\n",
    "        # find actual text\n",
    "        reviews = soup.find_all(\"div\", class_ = \"a-section review\")\n",
    "\n",
    "        # loop over all reviews\n",
    "        for review in reviews:\n",
    "            review_text = review.find(\"span\", class_ = \"a-size-base review-text\").text\n",
    "            reviews_list.append(review_text)\n",
    "\n",
    "\n",
    "    # print reviews_list\n",
    "\n",
    "    # make a dictionary of all the words in the reviews\n",
    "    dict_tags = {}\n",
    "\n",
    "    for reviews in reviews_list:\n",
    "        tokens = nltk.word_tokenize(reviews)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "        for tags in tagged:\n",
    "            tag_text = tags[0]\n",
    "            tag_type = tags[1]\n",
    "\n",
    "            # only consider parts of speech in [list_of_tags]\n",
    "            if tag_type in list_of_tags:\n",
    "\n",
    "                if tag_text in dict_tags:\n",
    "                    dict_tags[tag_text] += 1\n",
    "                else:\n",
    "                    dict_tags[tag_text] = 1\n",
    "\n",
    "    dict_sorted = sorted(dict_tags.items(), key=lambda x:x[1], reverse = True) # descending\n",
    "\n",
    "    # sum find of frequencies\n",
    "    sum_freq = float(sum(dict_tags.values()))\n",
    "\n",
    "    rel_freq_list = []\n",
    "\n",
    "    # calculate relative frequencies\n",
    "    for tags in dict_sorted:\n",
    "        tags_text = ( round(tags[1]/sum_freq, 4) )\n",
    "        rel_freq_list.append( (tags[0], tags_text) )\n",
    "\n",
    "    return rel_freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dic = word_freq('Harry Potter and the Deathly Hallows', 'JK Rowling', ['NNP', 'NNPS']) #only proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Harry', 0.1887),\n",
       " (u'Potter', 0.103),\n",
       " (u'Rowling', 0.0443),\n",
       " (u'Voldemort', 0.0212),\n",
       " (u'Hogwarts', 0.0141),\n",
       " (u'J.K.', 0.014),\n",
       " (u'Dumbledore', 0.0136),\n",
       " (u'Great', 0.0129),\n",
       " (u'Hallows', 0.0124),\n",
       " (u'JK', 0.0124),\n",
       " (u'Book', 0.0122),\n",
       " (u'Ron', 0.0121),\n",
       " (u'Phoenix', 0.0116),\n",
       " (u'Deathly', 0.0116),\n",
       " (u'Hermione', 0.0108),\n",
       " (u'Dale', 0.0101),\n",
       " (u'Jim', 0.0096),\n",
       " (u'Snape', 0.0089),\n",
       " (u'Lord', 0.0083),\n",
       " (u'HP', 0.0079)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysing the top 20 entries\n",
    "dic[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we analyze the above let us clean this data up a bit by combining terms that convey the same meaning:\n",
    "1. 'Harry', 'Potter', and 'HP' all correspond to Harry Potter, the titular character (also in the book title). So, we can replace these 3 terms with one super term having frequency = 0.2996\n",
    "2. Similarly 'Rowling', 'JK' and 'J.K.' all correspond to JK Rowling, the author of the series. The new frequency is 0.0707\n",
    "3. 'Voldemort' and 'Lord' both refer to Lord Voldemort, the series' antagonist. New frequency = 0.0295\n",
    "4. 'Deathly' and 'Hallows' correspond to the Deathly Hallows, the trio of power (also in the book title). New frequency = 0.024\n",
    "\n",
    "Thus the top 5 entries are:\n",
    "1. Harry Potter - 29.86\n",
    "2. JK Rowling - 0.0707\n",
    "3. Voldemort - 0.0295\n",
    "4. Deathly Hallows - 0.024\n",
    "5. Hogwarts - 0.0141\n",
    "\n",
    "These entries make sense. Given that the last book talks about how Harry discovers the Deathly Hallows and eventually defeats Voldemort after the Battle of Hogwarts and how it seems natural to also mention the author in the review (as meta-data for the book), our analysis seems pretty good! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic2 = word_freq('Harry Potter and the Deathly Hallows', 'JK Rowling', ['JJ', 'RB']) # adjectives and adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u\"n't\", 0.0407),\n",
       " (u'not', 0.0384),\n",
       " (u'so', 0.0252),\n",
       " (u'great', 0.0218),\n",
       " (u'good', 0.0191),\n",
       " (u'just', 0.0191),\n",
       " (u'very', 0.0176),\n",
       " (u'much', 0.0159),\n",
       " (u'first', 0.014),\n",
       " (u'really', 0.0131),\n",
       " (u'even', 0.0118),\n",
       " (u'last', 0.0118),\n",
       " (u'many', 0.0116),\n",
       " (u'other', 0.0116),\n",
       " (u'only', 0.0111),\n",
       " (u'well', 0.0111),\n",
       " (u'again', 0.0095),\n",
       " (u'new', 0.0095),\n",
       " (u'as', 0.0095),\n",
       " (u'too', 0.0082)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic2[:20] # top 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the series and especially the book was a hit, it is not surprising that 'great' and 'good' are 3rd and 4th, taking 40% of the frequency values. This once again confirms our analysis was great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Future improvements\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "What is the biggest learning I got from this process? \n",
    "Experimentation is paramount! It was only after I used the Chrome Inspector that I found out where the actual information resided. Fiddling around was key.\n",
    "\n",
    "## Future improvements\n",
    "\n",
    "Better knowledge of NLP will enable me to analyse this text corpurus better.\n",
    "\n",
    "Machine learning can be used to better club words with similar meanings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
