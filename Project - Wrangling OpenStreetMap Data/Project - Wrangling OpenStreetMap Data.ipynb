{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Wrangle OpenStreetMap Data\n",
    "\n",
    "Often the main part of a data scientists's role is to clean and wrangle the data into more accessible formats. While tedious and time-consuming, looking at the data at this early stages makes way for fewer inconsistencies later onwards.\n",
    "\n",
    "Here, I have decide to use the OpenStreetMap data for my home city of New Delhi. \n",
    "New Delhi is the capital of India, a country in South Asia.\n",
    "\n",
    "Note that the download is for a region known as NCR (National Capital Region), a set of places that includes New Delhi, and other adjoining places such as Purani Dilli (Old Delhi) and Gurgaon\n",
    "\n",
    "As the name suggests, the OpenStreetMap project is an open-source project and comprises people tagging physical locations as XML tags, giving key details such as latitude and longitude. Other key details such as the type of the place ie amenity/hospital/highway is also specified, with its specific name as key-value pairs.\n",
    "\n",
    "Consequently, the aim of this project is to explore the OpenStreetMap data, to clean the downloaded parts of the Delhi dataset, and find some features of the data.\n",
    "\n",
    "There are parts to this data analysis:\n",
    "1. Converting the XML data to CSV format\n",
    "    1. Spotting errors\n",
    "    2. Cleaning the data before conversion\n",
    "    3. Converting the data into a CSV file\n",
    "    4. Importing the CSV data into a MySQL database\n",
    "      \n",
    "2. Finding basic statistics about the data\n",
    "\n",
    "3. Future improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries that will be used \n",
    "\n",
    "# for SQL connections to the database\n",
    "import MySQLdb\n",
    "\n",
    "# for parsing the XML tree (library is built with C) and converting it into the CSV\n",
    "import xml.etree.cElementTree as ET\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "# validator library\n",
    "import cerberus\n",
    "\n",
    "#validation schema\n",
    "import schema_osm\n",
    "\n",
    "# dictionary module for simplifying life\n",
    "from collections import defaultdict\n",
    "\n",
    "# pretty printing\n",
    "import pprint\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# measuring time periods\n",
    "import time\n",
    "\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# osm file to convert\n",
    "OSM_FILE = 'new-delhi_01.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Converting the XML data to CSV format\n",
    "\n",
    "Before we convert the data, let us take a look at some sample data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_xml_tags(counter):\n",
    "    '''\n",
    "    view [counter] lines of xml_tags\n",
    "    '''\n",
    "    \n",
    "    num_of_lines = 0\n",
    "\n",
    "    for event, elem in ET.iterparse(OSM_FILE, events=(\"start\",)):\n",
    "        if (num_of_lines <= counter):\n",
    "            num_of_lines += 1\n",
    "            print elem.tag\n",
    "            print elem.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "osm\n",
      "{'timestamp': '2017-06-21T15:01:37Z', 'version': '0.6', 'generator': 'osmconvert 0.8.8'}\n",
      "bounds\n",
      "{'minlat': '28.183', 'maxlon': '77.733', 'minlon': '76.692', 'maxlat': '28.969'}\n",
      "node\n",
      "{'changeset': '46299189', 'uid': '3029661', 'timestamp': '2017-02-22T08:42:55Z', 'lon': '77.2159562', 'version': '53', 'user': 'saikabhi', 'lat': '28.6138967', 'id': '16173236'}\n",
      "tag\n",
      "{'k': 'admin_level', 'v': '2'}\n",
      "tag\n",
      "{'k': 'capital', 'v': 'yes'}\n",
      "tag\n",
      "{'k': 'is_capital', 'v': 'country'}\n",
      "tag\n",
      "{'k': 'is_in', 'v': 'National Capital Region, NCR, India'}\n",
      "tag\n",
      "{'k': 'is_in:continent', 'v': 'Asia'}\n",
      "tag\n",
      "{'k': 'is_in:country', 'v': 'India'}\n",
      "tag\n",
      "{'k': 'is_in:country_code', 'v': 'IN'}\n",
      "tag\n",
      "{'k': 'is_in:iso_3166_2', 'v': 'IN-DL'}\n"
     ]
    }
   ],
   "source": [
    "# view first 10 tags\n",
    "view_xml_tags(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(n, iterable):\n",
    "    '''\n",
    "    Return n items of the iterable as a list\n",
    "    '''\n",
    "    pprint.pprint (list(islice(iterable, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, the structure of the document is a bit more clear to you. Now, let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1A. Cleaning the data before conversion\n",
    "\n",
    "In order to spot issues with the data, we will consider a small segment of the data. This is because the actual file is 725 MB in size and doing any sort of basic calculations will be very time intensive!\n",
    "\n",
    "For this analysis, I will be dealing with 10,000 tags as my small segment. \n",
    "While this may not look \"small\" to you at all, it is certainly MUCH smaller than the number of tags in 0.7 GB of data, and also a good number to be a representative sample of what the data contains.\n",
    "\n",
    "The next cells list a few functions that I used to spot anomalies in the data. Their functionalities are listed in the doc strings below the function header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TAGS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keys(num_tags):\n",
    "    '''\n",
    "    Function that returns a dictionary of the first [num_tags] 'key' attributes of \"tag\" tags.\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    dic_keys = {} \n",
    "    \n",
    "    if (count <= num_tags):\n",
    "        for event, elem in ET.iterparse(OSM_FILE, events=(\"start\",)):\n",
    "            count += 1\n",
    "            if elem.tag == \"tag\":\n",
    "                \n",
    "                # check if in map\n",
    "                if 'k' and 'v' in elem.attrib:\n",
    "                    map_key = elem.attrib['k']\n",
    "                    if map_key in dic_keys:\n",
    "                        dic_keys[map_key] += 1\n",
    "                    else:\n",
    "                        dic_keys[map_key] = 1\n",
    "    return dic_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_keys = get_keys(NUM_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('name:kg', 1),\n",
      " ('maxspeed', 1447),\n",
      " ('snowmobile', 14),\n",
      " ('IRrouterank', 29),\n",
      " ('is_in', 292),\n",
      " ('name:bat-smg', 2),\n",
      " ('name:kv', 1),\n",
      " ('created_by', 254),\n",
      " ('to', 9),\n",
      " ('source:tracer', 3)]\n"
     ]
    }
   ],
   "source": [
    "sample(10, dic_keys.iteritems())\n",
    "\n",
    "# To see complete output, uncomment the next line\n",
    "# pprint.pprint(dic_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_values(num_tags):\n",
    "    '''\n",
    "    Function that returns a dictionary of the first [num_tags] 'value' attributes of \"tag\" tags.\n",
    "    '''\n",
    "\n",
    "    count = 0\n",
    "    dic_values = {}\n",
    "\n",
    "    if (count <= num_tags):\n",
    "        for event, elem in ET.iterparse(OSM_FILE, events=(\"start\",)):\n",
    "            count += 1\n",
    "            if elem.tag == \"tag\":\n",
    "                \n",
    "                # check if in map\n",
    "                if 'k' and 'v' in elem.attrib:\n",
    "                    map_key = elem.attrib['v']\n",
    "                    if map_key in dic_values:\n",
    "                        dic_values[map_key] += 1\n",
    "                    else:\n",
    "                        dic_values[map_key] = 1\n",
    "                        \n",
    "    return dic_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_values = get_values(NUM_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 3),\n",
      " ('110005', 5),\n",
      " ('110006', 12),\n",
      " ('110007', 2),\n",
      " ('110001', 24),\n",
      " ('110002', 12),\n",
      " ('110003', 15),\n",
      " ('Patel Nagar Railway Station', 1),\n",
      " ('Tihar Jail', 2),\n",
      " ('110009', 13)]\n"
     ]
    }
   ],
   "source": [
    "sample(10, dic_values.iteritems())\n",
    "\n",
    "# To see complete output, uncomment the next line\n",
    "# pprint.pprint(dic_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_alpha_values(dic):\n",
    "    '''\n",
    "    Function that returns a dictionary of only the alphabetical keys of dictionary [dic].\n",
    "    '''\n",
    "\n",
    "    dic_alpha = defaultdict(set)\n",
    "    \n",
    "    for values in dic:\n",
    "        values_lower = values.lower()\n",
    "        if values_lower.isalpha(): \n",
    "            first_letter = values_lower[0]\n",
    "            dic_alpha[first_letter].add(values)\n",
    "\n",
    "    return dic_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_dic_values = get_alpha_values(dic_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To see complete output, uncomment the next line\n",
    "# pprint.pprint(alpha_dic_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_tag(attr_name, search_string, number_of_results = 5, number_of_searches = NUM_TAGS):\n",
    "    '''\n",
    "    Function that finds and prints tags whose attribute [attr_name] contains the string [search_string] as long \n",
    "    as the number of results is fewer than [number_of_results] and the number of tags searched is \n",
    "    fewer than [number_of_searches].\n",
    "    \n",
    "    The found tag attributes and its parent tag are printed for ease of access.\n",
    "    '''\n",
    "    \n",
    "    results = 0\n",
    "    searches = 0\n",
    "    flag = 0\n",
    "        \n",
    "    for event, elem in ET.iterparse(OSM_FILE, events=(\"start\",)):\n",
    "        if elem.tag == \"node\":\n",
    "            for children in elem:\n",
    "                if results < number_of_results and searches < number_of_searches:\n",
    "\n",
    "                # checking the conditions listed in the doc string\n",
    "                    searches += 1\n",
    "\n",
    "                    if children.attrib[attr_name].find(search_string) != -1 :\n",
    "\n",
    "                        print children.attrib\n",
    "                        print elem.attrib\n",
    "\n",
    "                        results += 1\n",
    "                else:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values that were problematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inconsistent time formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'hour_on', 'v': '06:00'}\n",
      "{'changeset': '37968208', 'uid': '17429', 'timestamp': '2016-03-21T06:03:04Z', 'lon': '77.0342984', 'version': '4', 'user': 'thevikas', 'lat': '28.4453755', 'id': '271373824'}\n",
      "{'k': 'hour_off', 'v': '10:00'}\n",
      "{'changeset': '37968208', 'uid': '17429', 'timestamp': '2016-03-21T06:03:04Z', 'lon': '77.0342984', 'version': '4', 'user': 'thevikas', 'lat': '28.4453755', 'id': '271373824'}\n",
      "{'k': 'opening_hours', 'v': '24/7'}\n",
      "{'changeset': '2908758', 'uid': '26562', 'timestamp': '2009-10-21T05:35:56Z', 'lon': '77.0913044', 'version': '1', 'user': 'Nishant Sharma', 'lat': '28.489188', 'id': '537504262'}\n",
      "{'k': 'opening_hours', 'v': '24/7'}\n",
      "{'changeset': '2908758', 'uid': '26562', 'timestamp': '2009-10-21T05:35:57Z', 'lon': '77.0911757', 'version': '1', 'user': 'Nishant Sharma', 'lat': '28.4891315', 'id': '537504263'}\n",
      "{'k': 'opening_hours', 'v': '24/7'}\n",
      "{'changeset': '2915115', 'uid': '26562', 'timestamp': '2009-10-21T19:09:16Z', 'lon': '77.0860422', 'version': '1', 'user': 'Nishant Sharma', 'lat': '28.4809232', 'id': '537994220'}\n",
      "{'k': 'name:en', 'v': 'trekking equipment'}\n",
      "{'changeset': '44177150', 'uid': '3902918', 'timestamp': '2016-12-05T09:04:44Z', 'lon': '77.2782886', 'version': '2', 'user': u'Pay\\xe2m kharam\\xe8sh', 'lat': '28.5390089', 'id': '786590768'}\n"
     ]
    }
   ],
   "source": [
    "time_keys = find_tag('k', 'hour')\n",
    "time_values = find_tag('v', 'pm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one looks at the time formats, you can see that they are all over the place: some have spaces between the A and M, some have lowercase, while others don't have this format altogether! There are also values of \"24\", which I take to be \"24/7\" by infering from nearby values. It is also very apparent that the values of the key of \"opening_hours\" contain all the required time values.\n",
    "\n",
    "Thus, for standardization, I have chosen to use the 12-hour format (with AM and PM in uppercase and without the period between the individual letters), because I feel it is the easiest to read. The opening and closing times will be separated by a dash(-), for example: 10 AM - 4 PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Minor spelling errors\n",
    "\n",
    "Because of the difference in spoken English and Hindi (the other major language spoken throughout New Delhi), there have been minor spelling errors such as \"Dharampur\" (a locality) and \"Dharampura\" with the extra \"a\". \n",
    "\n",
    "To find all such values, I wrote a crude string similarity function that finds such pairs of words. Keep in mind that finding similarity between strings is an active area of research, and that is is just a very basic function, so it might find certain words that are not as \"similar\" as we would want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_sim(str1, str2):\n",
    "    '''\n",
    "    Crude similarity function for strings.\n",
    "    Compares letter values at each string's positions and returns a ratio of similarity.\n",
    "    If the ratio is greater than than a certain threshold, then the function outputs True, else False.\n",
    "    '''\n",
    "    \n",
    "    lst1 = list(str1)\n",
    "    lst2 = list(str2)\n",
    "    \n",
    "    lst1_no_spaces = []\n",
    "    lst2_no_spaces = []\n",
    "    \n",
    "    problem_chars = [\" \", \"_\"]\n",
    "    \n",
    "    for elem in lst1:\n",
    "         if elem not in problem_chars:\n",
    "            lst1_no_spaces.append(elem)\n",
    "    \n",
    "    for elem in lst2:\n",
    "        if elem not in problem_chars:\n",
    "            lst2_no_spaces.append(elem)\n",
    "                \n",
    "    smaller = min ( len(lst1_no_spaces), len(lst2_no_spaces) )\n",
    "    \n",
    "    if smaller == len(lst1_no_spaces):\n",
    "        smaller_lst = lst1_no_spaces\n",
    "        larger_lst = lst2_no_spaces\n",
    "    else:\n",
    "        smaller_lst = lst2_no_spaces\n",
    "        larger_lst = lst1_no_spaces\n",
    "        \n",
    "    for i in range(len(larger_lst) - len(smaller_lst)):\n",
    "        smaller_lst.append('X')\n",
    "                \n",
    "    count = 0\n",
    "    \n",
    "    if smaller > 3:\n",
    "        for elem1, elem2 in zip(lst1_no_spaces, lst2_no_spaces):\n",
    "        # print elem1, elem2\n",
    "            if elem1 == elem2:\n",
    "                count += 1\n",
    "    \n",
    "    return count/float(len(larger_lst)) >= 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_similar(dic_values, num_results = 10, num_searches = NUM_TAGS):\n",
    "    '''\n",
    "    Function to iterate through dictionary values of [dic_values] to find all\n",
    "    similar values and print them.\n",
    "    '''\n",
    "\n",
    "    searches = 0\n",
    "    results = 0\n",
    "\n",
    "    for values_1 in dic_values:\n",
    "        for values_2 in dic_values:\n",
    "            \n",
    "            searches += 1\n",
    "            # make sure the strings are alphabetical and start with the same letter: this ensures some smartness\n",
    "            # in terms of the algorithms\n",
    "            if (values_1.isalpha() and values_2.isalpha() and values_1 != values_2 and values_1[0] == values_2[0]):\n",
    "                \n",
    "                if (searches <= num_searches and results <= num_results):\n",
    "                    \n",
    "                    if string_sim(values_1, values_2):\n",
    "                    \n",
    "                        print \"Value 1:\", values_1\n",
    "                        print \"Value 2:\", values_2\n",
    "                        results += 1\n",
    "                    \n",
    "                else:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_similar(dic_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, even though my function is pretty mediocre, it has found quite a few values, for example:\n",
    "1. \"Bhuapur\" and \"Bhoapur\"\n",
    "2. \"Narender\" and \"Nardendra\"\n",
    "3. \"Electornics\" and \"Electronics\"\n",
    "4. \"Titarpur\" and \"Tatarpur\"\n",
    "5. \"Hindiston\", \"Hindıstan\" and \"Hindistan\"\n",
    "6. \"Rampur\" and \"Rampura\"\n",
    "\n",
    "While there are many more, the problem with this approach is that it will have to be solved manually, and not programmatically, because one does not know *a priori* which values will be misspelt and what the misspelling will look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the above problem, I dug deeper to find a simpler connection between the spellings. I noticed that the anglacized names of places in Hindi add an extra 'a' for better pronunciation, and so I set out to find such pairs of words. \n",
    "\n",
    "The advantage of narrowing down the differences in the words is that this particular problem can be fixed programmatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 1: Sunpur\n",
      "Value 2: Sunpura\n",
      "Value 1: Susheel\n",
      "Value 2: Susheela\n",
      "Value 1: Nathupur\n",
      "Value 2: Nathupura\n",
      "Value 1: Dharampur\n",
      "Value 2: Dharampura\n",
      "Value 1: Rampur\n",
      "Value 2: Rampura\n"
     ]
    }
   ],
   "source": [
    "extra_chars = ['a']\n",
    "\n",
    "for values_1 in dic_values:\n",
    "    for values_2 in dic_values:\n",
    "            \n",
    "        for endings in extra_chars:\n",
    "            if values_2 == values_1 + endings:\n",
    "\n",
    "                print \"Value 1:\", values_1\n",
    "                print \"Value 2:\", values_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can look to fix the places differing by an extra 'a'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Misformated strings: example - DTC\n",
    "\n",
    "The Delhi Transport Corporation is one of Delhi's biggest bus operators. \n",
    "\n",
    "While going through the XML file, I found that the DTC was written both as its acronym (DTC) and its full version (Delhi Transport Corporation). Because this is a globally accessible dataset, I decided to keep the full version (which includes the acronym) so that this makes more sense to people who decide to investigate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'operator', 'v': 'DTC'}\n",
      "{'changeset': '30470980', 'uid': '1751211', 'timestamp': '2015-04-25T10:14:28Z', 'lon': '77.1656102', 'version': '2', 'user': 'ajantriks', 'lat': '28.5461287', 'id': '566993158'}\n",
      "{'k': 'operator', 'v': 'DTC'}\n",
      "{'changeset': '30467334', 'uid': '1751211', 'timestamp': '2015-04-25T06:14:21Z', 'lon': '77.1682489', 'version': '2', 'user': 'ajantriks', 'lat': '28.5493627', 'id': '566993164'}\n",
      "{'k': 'name', 'v': 'Kashmiri Gate DTC Terminus'}\n",
      "{'changeset': '5961686', 'uid': '1306', 'timestamp': '2010-10-05T14:25:22Z', 'lon': '77.2283796', 'version': '1', 'user': 'PlaneMad', 'lat': '28.6685915', 'id': '938534499'}\n",
      "{'k': 'name', 'v': 'Sector 10 DTC bust stop'}\n",
      "{'changeset': '6400961', 'uid': '372318', 'timestamp': '2010-11-18T15:56:23Z', 'lon': '77.0565582', 'version': '1', 'user': 'rkumar02', 'lat': '28.5898321', 'id': '993723518'}\n",
      "{'k': 'name', 'v': 'DTC Depot'}\n",
      "{'changeset': '13779743', 'uid': '56597', 'timestamp': '2012-11-06T22:20:37Z', 'lon': '77.0536523', 'version': '2', 'user': 'Oberaffe', 'lat': '28.5814409', 'id': '995631896'}\n"
     ]
    }
   ],
   "source": [
    "find_tag('v', 'DTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values that seemed problematic, but were alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'source', 'v': 'AND'}\n",
      "{'changeset': '12138166', 'uid': '56597', 'timestamp': '2012-07-07T09:21:32Z', 'lon': '76.7811767', 'version': '3', 'user': 'Oberaffe', 'lat': '28.2048836', 'id': '245764539'}\n",
      "{'k': 'source', 'v': 'AND'}\n",
      "{'changeset': '15964016', 'uid': '91490', 'timestamp': '2013-05-03T18:37:27Z', 'lon': '76.7534322', 'version': '3', 'user': 'Heinz_V', 'lat': '28.2054048', 'id': '245764549'}\n",
      "{'k': 'source', 'v': 'AND'}\n",
      "{'changeset': '12067458', 'uid': '56597', 'timestamp': '2012-06-30T16:19:37Z', 'lon': '77.1076424', 'version': '3', 'user': 'Oberaffe', 'lat': '28.23457', 'id': '245764666'}\n",
      "{'k': 'source', 'v': 'AND'}\n",
      "{'changeset': '37945233', 'uid': '439726', 'timestamp': '2016-03-19T20:37:11Z', 'lon': '77.6588646', 'version': '2', 'user': 'chandusekharreddy', 'lat': '28.2584839', 'id': '245764757'}\n",
      "{'k': 'source', 'v': 'AND'}\n",
      "{'changeset': '3360495', 'uid': '17429', 'timestamp': '2009-12-13T05:44:23Z', 'lon': '77.1239532', 'version': '3', 'user': 'thevikas', 'lat': '28.2657839', 'id': '245764765'}\n"
     ]
    }
   ],
   "source": [
    "find_tag('v', 'AND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While glancing through the XML document, I found a lot of key-value pairs of \"source\" and \"AND\". \n",
    "Bewildered, I was sure I had spotted a problem, because \"AND\" did not seem correct at all to me.\n",
    "\n",
    "However, some searching on the <a href = \"http://wiki.openstreetmap.org/wiki/Key:source\">OSM Wiki</a> revealed otherwise.\n",
    "\n",
    "Under common \"source\" values, I found a <a href = \"http://wiki.openstreetmap.org/wiki/AND_Data\">link</a> to the \"AND\" value, which states, \"On the 4th of July 2007 AND Automotive Navigation Data donated the entire streetmap of the Netherlands as well as road networks for China and India... We also have the basic India road network and city names AND data in place...\".\n",
    "\n",
    "AHA! This makes complete sense now! The source of the data was through the AND dataset! This is infact important meta-data that should in fact be retained. \n",
    "\n",
    "\"Potlatch 0.9a\" is another software used that has been used as a source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'created_by', 'v': 'JOSM'}\n",
      "{'changeset': '19305', 'uid': '17429', 'timestamp': '2008-12-01T16:19:34Z', 'lon': '77.0576899', 'version': '9', 'user': 'thevikas', 'lat': '28.4690552', 'id': '249077809'}\n",
      "{'k': 'created_by', 'v': 'JOSM'}\n",
      "{'changeset': '19305', 'uid': '17429', 'timestamp': '2008-12-01T16:19:35Z', 'lon': '77.0703931', 'version': '7', 'user': 'thevikas', 'lat': '28.4792124', 'id': '249077810'}\n",
      "{'k': 'created_by', 'v': 'JOSM'}\n",
      "{'changeset': '508147', 'uid': '17429', 'timestamp': '2008-05-24T09:07:39Z', 'lon': '77.0701475', 'version': '4', 'user': 'thevikas', 'lat': '28.4592792', 'id': '249132367'}\n",
      "{'k': 'created_by', 'v': 'JOSM'}\n",
      "{'changeset': '500264', 'uid': '17429', 'timestamp': '2008-05-22T19:50:30Z', 'lon': '77.0583362', 'version': '3', 'user': 'thevikas', 'lat': '28.4680855', 'id': '249132371'}\n",
      "{'k': 'created_by', 'v': 'JOSM'}\n",
      "{'changeset': '1779803', 'uid': '138012', 'timestamp': '2009-07-09T11:34:43Z', 'lon': '77.0658862', 'version': '3', 'user': 'thepatel', 'lat': '28.4623384', 'id': '249221479'}\n"
     ]
    }
   ],
   "source": [
    "find_tag('v', 'JOSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"JOSM\" value was another one that confused me. With a key - value of \"created_by\": \"JOSM\", I was pretty sure that this was redundant information, since the parent \"node\" tag for each \"tag\" tag already contained the user who contributed that point. \n",
    "\n",
    "dispHowever, once I Googled this term, I found to my surprise that JOSM in fact an editor for OSM Maps for <a href = \"https://josm.openstreetmap.de/\">J</a>ava. As this is important information about the points, it should be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_tags(num = 20):\n",
    "\n",
    "    num_results = 0\n",
    "\n",
    "    for event, elem in ET.iterparse(OSM_FILE, events=(\"start\",)):\n",
    "        if (count <= NUM_TAGS):\n",
    "            if elem.tag == \"node\":\n",
    "                for children in elem:\n",
    "                    if num_results <= num:\n",
    "                        if len(children.attrib[attr]) == 2 or not children.attrib[attr].isalnum():\n",
    "                            print children.attrib\n",
    "                            num_results += 1\n",
    "\n",
    "                    else:\n",
    "                        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'admin_level', 'v': '2'}\n",
      "{'k': 'is_capital', 'v': 'country'}\n",
      "{'k': 'is_in', 'v': 'National Capital Region, NCR, India'}\n",
      "{'k': 'is_in:continent', 'v': 'Asia'}\n",
      "{'k': 'is_in:country', 'v': 'India'}\n",
      "{'k': 'is_in:country_code', 'v': 'IN'}\n",
      "{'k': 'is_in:iso_3166_2', 'v': 'IN-DL'}\n",
      "{'k': 'name:ace', 'v': 'New delhi'}\n",
      "{'k': 'name:af', 'v': 'Nieu-Delhi'}\n",
      "{'k': 'name:am', 'v': u'\\u1292\\u12cd \\u12f4\\u120a'}\n",
      "{'k': 'name:an', 'v': 'Nueva Delhi'}\n",
      "{'k': 'name:ang', 'v': u'N\\u012b\\u01bfe Delhi'}\n",
      "{'k': 'name:ar', 'v': u'\\u0646\\u064a\\u0648\\u062f\\u0644\\u0647\\u064a'}\n",
      "{'k': 'name:bat-smg', 'v': u'Naujas\\u0117s Del\\u0117s'}\n",
      "{'k': 'name:be', 'v': u'\\u041d\\u044c\\u044e-\\u0414\\u044d\\u043b\\u0456'}\n",
      "{'k': 'name:be-tarask', 'v': u'\\u041d\\u044c\\u044e-\\u0414\\u044d\\u043b\\u0456'}\n",
      "{'k': 'name:bg', 'v': u'\\u041d\\u044e \\u0414\\u0435\\u043b\\u0445\\u0438'}\n",
      "{'k': 'name:bn', 'v': u'\\u09a8\\u09a4\\u09c1\\u09a8 \\u09a6\\u09bf\\u09b2\\u09cd\\u09b2\\u09bf'}\n",
      "{'k': 'name:bo', 'v': u'\\u0f53\\u0f7a\\u0f60\\u0f74\\u0f0b\\u0f51\\u0f72\\u0f63\\u0f0b\\u0f63\\u0f72\\u0f0d'}\n",
      "{'k': 'name:bpy', 'v': u'\\u09a8\\u09c1\\u09f1\\u09be \\u09a6\\u09bf\\u09b2\\u09cd\\u09b2\\u09c0'}\n",
      "{'k': 'name:br', 'v': 'New Delhi'}\n"
     ]
    }
   ],
   "source": [
    "display_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was trying to find problematic values: in general, shorter and non-alpha numeric values are more likely to be problematic. \n",
    "\n",
    "If you look above at the data, you will see most of these \"problematic\" values are unicode characters (\\u values).\n",
    "However, you will see that these are in fact names of India in different languages, as can be seen in the key-value pairs: \"name:XX\" : \"\\u1292\\u12cd \\u12f4\\u120a\". \n",
    "\n",
    "Because India is a diverse country with more than 600 regional languages, this kind of <b>linguistic difference</b> makes sense.\n",
    "\n",
    "Another language related problem was the spellings of \"India\" such as \"Indya\" and \"Indija\". \n",
    "Once again, this is not an error, and simply a translation of the regional spellings of India into English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Cleaning the data before conversion\n",
    "Now that we have identified quite a few problems with the data, let us tackle them one by one!\n",
    "\n",
    "### 1. Inconsistent time formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'opening_hours', 'v': '24/7'}\n",
      "{'changeset': '2908758', 'uid': '26562', 'timestamp': '2009-10-21T05:35:56Z', 'lon': '77.0913044', 'version': '1', 'user': 'Nishant Sharma', 'lat': '28.489188', 'id': '537504262'}\n",
      "{'k': 'opening_hours', 'v': '24/7'}\n",
      "{'changeset': '2908758', 'uid': '26562', 'timestamp': '2009-10-21T05:35:57Z', 'lon': '77.0911757', 'version': '1', 'user': 'Nishant Sharma', 'lat': '28.4891315', 'id': '537504263'}\n",
      "{'k': 'opening_hours', 'v': '24/7'}\n",
      "{'changeset': '2915115', 'uid': '26562', 'timestamp': '2009-10-21T19:09:16Z', 'lon': '77.0860422', 'version': '1', 'user': 'Nishant Sharma', 'lat': '28.4809232', 'id': '537994220'}\n",
      "{'k': 'opening_hours', 'v': '24/7'}\n",
      "{'changeset': '2915115', 'uid': '26562', 'timestamp': '2009-10-21T19:09:16Z', 'lon': '77.0860364', 'version': '1', 'user': 'Nishant Sharma', 'lat': '28.4810242', 'id': '537994221'}\n",
      "{'k': 'opening_hours', 'v': '24/7'}\n",
      "{'changeset': '2915115', 'uid': '26562', 'timestamp': '2009-10-21T19:09:16Z', 'lon': '77.0860307', 'version': '1', 'user': 'Nishant Sharma', 'lat': '28.4812617', 'id': '537994222'}\n"
     ]
    }
   ],
   "source": [
    "find_tag('k', 'opening_hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mappings from incorrect spellings to correct spellings \n",
    "# (based on sample output from above)\n",
    "\n",
    "update_times = {\n",
    "        '24': \"'24/7\", # extra apostrophe to prevent conversion of 24/7 to 24 July\n",
    "        'mo': 'Monday',\n",
    "        'mon': 'Monday',\n",
    "        'tu': 'Tuesday',\n",
    "        'we': 'Wednesday',\n",
    "        'wed' 'Wednesday'\n",
    "        'th': 'Thursday',\n",
    "        'thur': 'Thursday',\n",
    "        'fr': 'Friday',\n",
    "        'sa': 'Saturday',\n",
    "        'sat': 'Saturday',\n",
    "        'su': 'Sunday',\n",
    "        'sun': 'Sunday',\n",
    "        'a.m': 'AM',\n",
    "        'a.m.': 'AM',\n",
    "        'am': 'AM',\n",
    "        'p.m': 'PM',\n",
    "        'p.m.': 'PM',\n",
    "        'pm': 'PM',\n",
    "        'to': '-'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_times(word):\n",
    "    '''\n",
    "    Function to cleanup the time format as per standards listed in above cells\n",
    "    '''    \n",
    "    try:\n",
    "    \n",
    "    # separate characters such as '10am' to obtain '10 am' for parsing\n",
    "        needs_spacing = re.compile('[0-9][a-z]')\n",
    "        space_pos = needs_spacing.findall(word)\n",
    "\n",
    "        for phrases in space_pos:\n",
    "            word_pos = word.find(phrases)\n",
    "            word = word[:word_pos + 1] + \" \" + word[word_pos + 1:]\n",
    "\n",
    "        # find all instances of 24 hour time and convert them\n",
    "        time_format = re.compile('[0-9]+:[0-9]+')\n",
    "        time_lst = time_format.findall(word)\n",
    "\n",
    "        for times in time_lst:\n",
    "\n",
    "            start = word.find(times)\n",
    "            length = len(times)\n",
    "            end = start + length\n",
    "\n",
    "            colon = times.find(\":\")\n",
    "            hour_int = int(times[:colon])\n",
    "            minute_str = times[colon:end]\n",
    "\n",
    "            if hour_int >= 12:\n",
    "                if hour_int != 12:\n",
    "                    hour_int -= 12\n",
    "                time_str = str(hour_int)\n",
    "                time_str += minute_str\n",
    "            else:\n",
    "                time_str = str(hour_int)\n",
    "                time_str += minute_str    \n",
    "\n",
    "            word = word[:start] + time_str + word[end:] \n",
    "\n",
    "        # separate characters such as '-' from individual words for better parsing\n",
    "        new_word = \"\"\n",
    "\n",
    "        for letter_pos in range(len(word)):\n",
    "\n",
    "            if word[letter_pos] in [\"-\", \",\"]:\n",
    "                if word[letter_pos - 1] != \" \" and word[letter_pos + 1] == \" \":\n",
    "                    new_word += \" \" + word[letter_pos]\n",
    "\n",
    "                elif word[letter_pos - 1] == \" \" and word[letter_pos + 1] != \" \":\n",
    "                    new_word += word[letter_pos] + \" \"\n",
    "\n",
    "                elif word[letter_pos - 1] != \" \" and word[letter_pos + 1] != \" \":\n",
    "                    new_word += \" \" + word[letter_pos] + \" \"\n",
    "\n",
    "            else:\n",
    "                new_word += word[letter_pos]\n",
    "\n",
    "        # update words as per dictionary values\n",
    "        sentence_lst = new_word.split()\n",
    "\n",
    "        answer_lst = []\n",
    "\n",
    "\n",
    "        for words in sentence_lst:\n",
    "\n",
    "            # lower\n",
    "            words = words.lower()\n",
    "\n",
    "            # check if in dict\n",
    "            if words in update_times:\n",
    "                answer_lst.append(update_times[words])\n",
    "            else:\n",
    "                answer_lst.append(words)\n",
    "\n",
    "        extra_spaces_word =  \" \".join(answer_lst)\n",
    "\n",
    "        final_word = \"\"\n",
    "\n",
    "        # remove extra spaces fromm commas\n",
    "        for letter_pos in range(len(extra_spaces_word)):\n",
    "            if extra_spaces_word[letter_pos] == \" \" and extra_spaces_word[letter_pos + 1] == \",\":\n",
    "                pass\n",
    "            elif extra_spaces_word[letter_pos] == \",\" and extra_spaces_word[letter_pos + 1] != \" \":\n",
    "                final_word += \", \"\n",
    "\n",
    "            else:\n",
    "                final_word += extra_spaces_word[letter_pos]\n",
    "\n",
    "        return final_word\n",
    "    \n",
    "    except:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 AM - 2 PM'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example use of function\n",
    "cleanup_times('10a.m. to 2pm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Minor spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_extra_a(str1, set_of_values):\n",
    "    '''\n",
    "    if [str1] has an extra \"a\", and its correct spellings has\n",
    "    been encountered, \"fix\" it\n",
    "    '''\n",
    "    try:\n",
    "        set_of_values.add(str1)\n",
    "    \n",
    "        if str1[-1] == \"a\" and (str1[:-1] in set_of_values):\n",
    "            return str1[:-1]\n",
    "    except:\n",
    "        pass\n",
    "    return str1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Misformatted string - example: DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_string_dtc(value):\n",
    "    return \"Delhi Transport Corporation ( DTC )\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Converting the data into a CSV file\n",
    "\n",
    "Now that we have successfully dealt with a few issues programmatically, and are aware of other factors, we are ready to convert!\n",
    "\n",
    "The function below converts AND validates the conversion (based on the schema in schema_osm.py). This ensures that the conversion that takes place is correct. \n",
    "\n",
    "Conversion is done on the ENTIRE file and may take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from Udacity course quiz and edited afterwards\n",
    "# note that cells in Excel may have weird formatting, but rest assured, they retain their values\n",
    "\n",
    "\"\"\"\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so you will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "\n",
    "## Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon\n",
    "        is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "The final return value for a \"node\" element should look something like:\n",
    "\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "       'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "      'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within\n",
    "            the way element\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "\n",
    "SCHEMA = schema_osm.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "NODE_ATTRIB = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAG_ATTRIB = ['k', 'v']\n",
    "WAY_ATTRIB = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SET_OF_VALUES = set()\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "    elem = element\n",
    "    \n",
    "    # print elem.tag\n",
    "    if elem.tag == \"node\":\n",
    "        node_attribs = {}\n",
    "        for attributes in NODE_ATTRIB:\n",
    "            node_attribs[attributes] = elem.attrib[attributes]\n",
    "                \n",
    "            # children \"tag\" elements\n",
    "            tags = []\n",
    "            \n",
    "        # deleting extraneous time info\n",
    "        extra_pos = node_attribs['timestamp'].find('T')\n",
    "        node_attribs['timestamp'] = node_attribs['timestamp'][:extra_pos]\n",
    "            \n",
    "    count = 0\n",
    "    for child in elem:\n",
    "        \n",
    "        if child.tag == \"tag\":    \n",
    "            node_tag_attrib_dict = {}\n",
    "\n",
    "            for attributes in NODE_TAG_ATTRIB:\n",
    "                \n",
    "                node_tag_attrib_dict[attributes] = child.attrib[attributes]   \n",
    "                node_tag_attrib_dict['id'] = elem.attrib['id']\n",
    "\n",
    "                # filter\n",
    "                colon_pos = child.attrib[\"k\"].find(\":\") \n",
    "                # problematic\n",
    "\n",
    "                if colon_pos == -1:\n",
    "                    node_tag_attrib_dict['type'] = \"regular\"\n",
    "                else:\n",
    "                    node_tag_attrib_dict['type'] = child.attrib[\"k\"][:colon_pos]\n",
    "                    node_tag_attrib_dict['k'] = child.attrib[\"k\"][colon_pos + 1:]\n",
    "\n",
    "            node_tag_attrib_dict['key'] = node_tag_attrib_dict['k']\n",
    "            del node_tag_attrib_dict['k']\n",
    "\n",
    "            node_tag_attrib_dict['value'] = node_tag_attrib_dict['v']\n",
    "            del node_tag_attrib_dict['v']\n",
    "               \n",
    "            if node_tag_attrib_dict['key'] == \"opening_hours\":\n",
    "                node_tag_attrib_dict['value'] = cleanup_times(node_tag_attrib_dict['value'])\n",
    "                \n",
    "            node_tag_attrib_dict['value'] = fix_extra_a(node_tag_attrib_dict['value'], SET_OF_VALUES)    \n",
    "            \n",
    "            if node_tag_attrib_dict['key'] == \"DTC\":\n",
    "                format_string_dtc(node_tag_attrib_dict['value'])\n",
    "\n",
    "            # add to list\n",
    "            tags.append(node_tag_attrib_dict)\n",
    "        \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file:\n",
    "#          codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "#          codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "#          codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "#         ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "#         way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "#         way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "#         ways_writer.writeheader()\n",
    "#         way_nodes_writer.writeheader()\n",
    "#         way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "\n",
    "process_map(OSM_FILE, validate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Importing the CSV data into a MySQL database\n",
    "\n",
    "Importing the CSV into MySQL is easy; we just need to ensure that there is a connection to the database and all tables, in which we want to put data, exist.\n",
    "\n",
    "Putting the data will take time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1L"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open database connection\n",
    "# Put your database credentials here\n",
    "USER = \"root\"\n",
    "PWORD = \"root\"\n",
    "\n",
    "db = MySQLdb.connect(\"localhost\", USER, PWORD)\n",
    "\n",
    "# prepare a cursor object using cursor() method\n",
    "cursor = db.cursor()\n",
    "\n",
    "# drop any table with the same name and create a new one\n",
    "sql = 'DROP DATABASE IF EXISTS osm'\n",
    "cursor.execute(sql)\n",
    "\n",
    "sql = 'CREATE DATABASE IF NOT EXISTS osm'\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql1 = '''CREATE TABLE IF NOT EXISTS nodes ( \n",
    "    id BIGINT PRIMARY KEY NOT NULL, \n",
    "    lat REAL, \n",
    "    lon REAL, \n",
    "    user TEXT, \n",
    "    uid BIGINT, \n",
    "    version BIGINT, \n",
    "    changeset BIGINT, \n",
    "    timestamp DATE \n",
    "    );'''\n",
    "\n",
    "# Because key is a keyword, it has to be enclosed in `key`\n",
    "sql2 = '''CREATE TABLE IF NOT EXISTS nodes_tags ( \n",
    "    id BIGINT,\n",
    "    `key` TEXT, \n",
    "    `value` TEXT, \n",
    "    type TEXT, \n",
    "    FOREIGN KEY (id) REFERENCES nodes(id) \n",
    "    );'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "sql_list = [sql1, sql2]\n",
    "for sqls in sql_list:\n",
    "    cursor.execute(sqls)\n",
    "    \n",
    "# close database\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted: 3406392\n",
      "Rejected: 0\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "# counters for accepted and rejected tags\n",
    "count_good = 0\n",
    "count_bad = 0\n",
    "\n",
    "csv_data = csv.reader(file('nodes.csv') )\n",
    "for row in csv_data:\n",
    "    \n",
    "    # prevent header from getting stored into the database\n",
    "    if row[0] == 'id':\n",
    "        continue\n",
    "        \n",
    "    # in case any error occurs\n",
    "    try:\n",
    "        iid = row[0]\n",
    "        lat = row[1]\n",
    "        lon = row[2]\n",
    "        user = row[3]\n",
    "        uid = row[4]\n",
    "        version = row[5]\n",
    "        changeset = row[6]\n",
    "        timestamp = row[7]\n",
    "\n",
    "        execution = 'INSERT INTO `nodes` (`id`, `lat`, `lon`, `user`, `uid`, `version`, `changeset`, `timestamp`) VALUES (' + iid + ', ' + lat + ', ' + lon + ', ' + '\"' + user + '\"' + ', ' + uid + ', ' + version + ', ' + changeset + ', ' + '\"' + timestamp + '\"' + ')'\n",
    "    \n",
    "        # print execution\n",
    "\n",
    "        cursor.execute(execution)\n",
    "        count_good += 1\n",
    "    \n",
    "    \n",
    "    except:\n",
    "        \n",
    "        # print bad input\n",
    "        print execution\n",
    "        count_bad += 1\n",
    "        \n",
    "\n",
    "db.commit()\n",
    "\n",
    "# close the connection to the database.\n",
    "cursor.close()\n",
    "\n",
    "print \"Accepted: \" + str(count_good)\n",
    "print \"Rejected: \" + str(count_bad)\n",
    "print \"Accuracy: \" + str(round(float(count_good)/(count_bad + count_good), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that out of the approximately 340 thousand tags, all made it to the database! Good data cleaning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO `nodes_tags` (`id`, `key`, `value`, `type`) VALUES (539314057, \"housenumber\", \"140\\\", \"addr\")\n",
      "INSERT INTO `nodes_tags` (`id`, `key`, `value`, `type`) VALUES (3832251782, \"fixme\", \"Yes bank is not in that corner. I found Yes bank near \"Centre for Policy Research\", more exactly: 28.601838 77.189462\", \"regular\")\n",
      "Accepted: 41401\n",
      "Rejected: 2\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Definitions of variable similar to above\n",
    "\n",
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "count_good = 0\n",
    "count_bad = 0\n",
    "\n",
    "csv_data = csv.reader(file('nodes_tags.csv') )\n",
    "for row in csv_data:\n",
    "    if row[0] == 'id':\n",
    "        continue\n",
    "    try:\n",
    "        iid = row[0]\n",
    "        key = row[1]\n",
    "        value = row[2]\n",
    "        types = row[3]\n",
    "\n",
    "        execution = 'INSERT INTO `nodes_tags` (`id`, `key`, `value`, `type`) VALUES ' + '(' + iid + ', ' + '\"' + key + '\"' + ', ' + '\"' + value + '\"' + ', ' + '\"' + types + '\"'+ ')'\n",
    "        # print execution\n",
    "\n",
    "        cursor.execute(execution)\n",
    "        db.commit()\n",
    "        count_good += 1\n",
    "\n",
    "    except:\n",
    "        print execution\n",
    "        count_bad += 1\n",
    "        \n",
    "    \n",
    "# close the connection to the database.\n",
    "cursor.close()\n",
    "\n",
    "print \"Accepted: \" + str(count_good)\n",
    "print \"Rejected: \" + str(count_bad)\n",
    "print \"Accuracy: \" + str(round(float(count_good)/(count_bad + count_good) , 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason behind the rejected tags:\n",
    "1. the first has a special character \\ in the \"140\\ \" that escapes the \\\n",
    "2. the second uses double quotes within a field, where double quotes are also used for delimiting certain fields.\n",
    "\n",
    "Even so, the number of errors is not huge, and comparable to what one can find in such datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Finding basic statistics about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of the file:\n",
    "1. 24.6 MB (compressed)\n",
    "2. 725 MB (uncompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of `way` tags\n",
    "\n",
    "This are none; hence, they are not even searched for in the XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of `node` tags\n",
    "\n",
    "3406392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3406392L,),)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT COUNT(*) FROM nodes'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "print result\n",
    "    \n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of `tag` tags\n",
    "\n",
    "41401\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((41401L,),)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT COUNT(*) FROM nodes_tags'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "print result\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of distinct users\n",
    "\n",
    "1311\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1311L,),)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT COUNT(DISTINCT user) FROM nodes'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "print result\n",
    "    \n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most active users\n",
    "\n",
    "Oberaffe with 223946 is the most active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Oberaffe', 223946L)\n",
      "('premkumar', 128796L)\n",
      "('saikumar', 127139L)\n",
      "('Naresh08', 121068L)\n",
      "('anushap', 110283L)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT user, COUNT(*) AS count FROM nodes GROUP BY user ORDER BY count DESC LIMIT 5'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for rows in result:\n",
    "    print rows\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most active time\n",
    "\n",
    "3 June 2015, with 163392 edits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.date(2015, 6, 3), 163392L)\n",
      "(datetime.date(2015, 6, 8), 158008L)\n",
      "(datetime.date(2015, 6, 5), 156841L)\n",
      "(datetime.date(2015, 6, 9), 151059L)\n",
      "(datetime.date(2015, 6, 4), 147670L)\n",
      "(datetime.date(2015, 5, 25), 145605L)\n",
      "(datetime.date(2015, 6, 2), 142810L)\n",
      "(datetime.date(2015, 6, 6), 135451L)\n",
      "(datetime.date(2015, 6, 1), 117400L)\n",
      "(datetime.date(2016, 5, 4), 102349L)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT timestamp, COUNT(*) AS count FROM nodes GROUP BY timestamp ORDER BY count DESC LIMIT 10'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for rows in result:\n",
    "    print rows\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of address edits\n",
    "\n",
    "4578"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('addr', 4578L)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT type, COUNT(*) AS count FROM nodes_tags GROUP BY type HAVING type = \"addr\"'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for rows in result:\n",
    "    print rows\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of `tag` edits\n",
    "\n",
    "tower: 16072\n",
    "\n",
    "\"towers\" here refers to towers on the electricity grid.\n",
    "\n",
    "IN: 5th -> Makes sense as \"IN\" refers to India\n",
    "\n",
    "Gurgaon: top 10 -> This is an adjoining area of New Delhi; this again makes sense as the map data contains information not only about New Delhi, but the National Capital Region.\n",
    "\n",
    "Similarly, '110006' (~ 30th) is the **postal code** for Chandni Chowk (not coincendentally the value below it on the list), a famous market in Old Delhi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tower', 16072L)\n",
      "('locality', 890L)\n",
      "('yes', 764L)\n",
      "('traffic_signals', 557L)\n",
      "('tree', 538L)\n",
      "('IN', 379L)\n",
      "('station', 347L)\n",
      "('street_vendor', 329L)\n",
      "('gate', 327L)\n",
      "('bus_stop', 279L)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT value, COUNT(*) AS count FROM nodes_tags GROUP BY value ORDER BY count DESC LIMIT 10'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for rows in result:\n",
    "    print rows\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most edits to Chandni Chowk (cross table query)\n",
    "\n",
    "From my above findings, I was curious to know who was adding all these locations in Chandni Chowk on the OSM dataset, so I queried for both the postal code and name (queries taken *verbatim* from the result list above)\n",
    "\n",
    "It looks like 'thevikas' (probably someone called Vikas) knows a lot about Old Delhi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('thevikas', 56L)\n",
      "('Oberaffe', 3L)\n",
      "('Jitin Kumar', 2L)\n",
      "('Claudio Acosta', 1L)\n",
      "('Usma Dhammei', 1L)\n",
      "('yoyi79', 1L)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT user, COUNT(*) AS count FROM nodes WHERE id in \\\n",
    "    (SELECT id FROM nodes_tags WHERE value = \"110006\" OR value = \"Lajpat Rai Market, Chandni Chowk, Delhi\") GROUP BY user ORDER BY count DESC'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for rows in result:\n",
    "    print rows\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of types of amenities\n",
    "\n",
    "As was expected, *frequently visited* amenities such as restaurants, ATMs, places of worship and banks are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('restaurant', 223L)\n",
      "('fuel', 213L)\n",
      "('atm', 202L)\n",
      "('place_of_worship', 175L)\n",
      "('bank', 169L)\n",
      "('school', 158L)\n",
      "('fast_food', 131L)\n",
      "('parking', 89L)\n",
      "('hospital', 88L)\n",
      "('cafe', 78L)\n",
      "('pharmacy', 71L)\n",
      "('police', 52L)\n",
      "('cinema', 43L)\n",
      "('embassy', 42L)\n",
      "('toilets', 41L)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT `value`, COUNT(*) AS count FROM nodes_tags WHERE `key` =  \"amenity\" GROUP BY `value` ORDER BY count DESC LIMIT 15'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for rows in result:\n",
    "    print rows\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of cafes\n",
    "\n",
    "78 mentions of \"cafe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cafe', 78L)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT `value`, COUNT(*) AS count FROM nodes_tags WHERE `key` =  \"amenity\" AND `value` = \"cafe\" GROUP BY `value`'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for rows in result:\n",
    "    print rows\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of education centres\n",
    "\n",
    "As can be seen, New Delhi seems to be an education hub, with 8 kindergartens, 26 colleges and a whopping 158 schools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('college', 26L)\n",
      "('kindergarten', 8L)\n",
      "('school', 158L)\n"
     ]
    }
   ],
   "source": [
    "db = MySQLdb.connect(\"localhost\", \"root\", \"root\", \"osm\")\n",
    "cursor = db.cursor()\n",
    "\n",
    "execution = 'SELECT `value`, COUNT(*) AS count FROM nodes_tags WHERE `key` =  \"amenity\" AND (`value` = \"college\" OR `value`  = \"school\" OR `value` = \"kindergarten\") GROUP BY `value`'\n",
    "cursor.execute(execution)\n",
    "db.commit()\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for rows in result:\n",
    "    print rows\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Future improvements\n",
    "\n",
    "Phew! We are finally done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However much we may have accomplished, there is still more to be done!\n",
    "\n",
    "Here are certain suggestions:\n",
    "1. 'is_in:XXX' occurs a lot as a key in key-value pairs. This strikes me as odd because the words after 'is_in' are self-explanatory, making 'is_in' redundant. \n",
    "2. Checking for common spelling errors: in my analysis, I come upon a lot of misspelt words such as \"SMOOTHNES\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'is_in', 'v': 'National Capital Region, NCR, India'}\n",
      "{'changeset': '46299189', 'uid': '3029661', 'timestamp': '2017-02-22T08:42:55Z', 'lon': '77.2159562', 'version': '53', 'user': 'saikabhi', 'lat': '28.6138967', 'id': '16173236'}\n",
      "{'k': 'is_in:continent', 'v': 'Asia'}\n",
      "{'changeset': '46299189', 'uid': '3029661', 'timestamp': '2017-02-22T08:42:55Z', 'lon': '77.2159562', 'version': '53', 'user': 'saikabhi', 'lat': '28.6138967', 'id': '16173236'}\n",
      "{'k': 'is_in:country', 'v': 'India'}\n",
      "{'changeset': '46299189', 'uid': '3029661', 'timestamp': '2017-02-22T08:42:55Z', 'lon': '77.2159562', 'version': '53', 'user': 'saikabhi', 'lat': '28.6138967', 'id': '16173236'}\n",
      "{'k': 'is_in:country_code', 'v': 'IN'}\n",
      "{'changeset': '46299189', 'uid': '3029661', 'timestamp': '2017-02-22T08:42:55Z', 'lon': '77.2159562', 'version': '53', 'user': 'saikabhi', 'lat': '28.6138967', 'id': '16173236'}\n",
      "{'k': 'is_in:iso_3166_2', 'v': 'IN-DL'}\n",
      "{'changeset': '46299189', 'uid': '3029661', 'timestamp': '2017-02-22T08:42:55Z', 'lon': '77.2159562', 'version': '53', 'user': 'saikabhi', 'lat': '28.6138967', 'id': '16173236'}\n"
     ]
    }
   ],
   "source": [
    "find_tag('k', 'is_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits and Anticipated Problems 1 are in regard to point 1 made above.\n",
    "\n",
    "Benefits and Anticipated Problems 2 are in regard to point 2 made above.\n",
    "\n",
    "### Benefits:\n",
    "1. This can be easily cleaned programmatically; however, the problem is that 'is_in' is ONE example of redundant tags. \n",
    "2. Easy to clean a certain subset of spelling mistakes programmatically! - see above\n",
    "\n",
    "### Anticipated Problems:\n",
    "1. Finding any more redundant tags still requires a manual sweep at first! :(\n",
    "2. However, as much as I caught certain typical errors, I am unable to catch errors for each and every word. Furthermore, in the current scope of the project, it is impossible to predict the next word in order to suggest improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both reasons above point to the use of machine learning, artificial intelligence and Natural Language Processing (NLP). With those tools in our arsenal, we could do much more sophisticated analysis and cleaning. Let me elaborate on what I said: NLP and ML in combination could be used to predict the most common misspellings of words. These could further act as training data for the model, and over time it would become better. Artificial intelligence could be used to smartly guess common patterns of dirty data. Thus, this trio of skills could be really very useful!\n",
    "   \n",
    "Another way to improve this dataset would be to gamify this experience: the more edits you make, the higher score you have. A feature that could be added to this is extra points for editing the place at which you're currently standing (making use of the GPS)!. These could seriously improve participation and improve this project to a large extent. \n",
    "    \n",
    "A last point would be to use an API like Google Maps. I did try this; however, because the API autocorrects incorrect spellings, it is of little use in determining if a place really exists."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
