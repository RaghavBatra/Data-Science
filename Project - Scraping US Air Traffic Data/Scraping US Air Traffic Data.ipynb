{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling: Scraping the Web\n",
    "\n",
    "## Introduction\n",
    "As part of my own research, fuelled by Udacity's course content on Data Wrangling, I decided to make use of my new-found skills to scrap a lot of the data of the US Bureau of Transportation Statistics, namely information about statistics regarding major airports and the carriers travelling through them.  \n",
    "\n",
    "The purpose of this project is not to answer questions about airports and planes; rather, the focus of the project is on understanding how to get the data, even when it's freely available as a webpage. No doubt, statistics about the airports and carriers can be carried, but that is a secondary focus.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "My analysis of this data is broken into 3 phases:\n",
    "1. Downloading the data to be scraped\n",
    "2. Scraping the data\n",
    "3. Converting the scraped data into a more accessible format\n",
    "4. Analysing the extracted data\n",
    "\n",
    "The importance of my analysis could not be underscored by the fact that it is imperative to **try the above steps on a single data element** and *only* then generalize the process. This has helped me catch errors early on, and made me more efficient.\n",
    "\n",
    "*Note: Cell output is restricted to 1000 characters for brevity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic imports for the rest of the analysis\n",
    "\n",
    "# making HTTPS requests\n",
    "import requests\n",
    "\n",
    "# HTML scraping and parsing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "# folder navigation\n",
    "import os\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# data intensive\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# sample page to scrape\n",
    "scrape_page = \"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\"\n",
    "\n",
    "s = requests.session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading the data to be scraped\n",
    "\n",
    "### Basics\n",
    "So, the first thing to be done is to take a look at what we're dealing with here. To do so, go to <a href = \"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\">this website</a>. \n",
    "\n",
    "When you go there, you will notice the following dropdown buttons for both carriers and airports:<br><br>\n",
    "<img src = \"Screenshot (109).png\" style = \"width: 60%; height: 60%\">\n",
    "<br>\n",
    "You will also see a table like so: <br><br>\n",
    "<img src = \"Screenshot (108).png\" style = \"width: 60%; height: 60%\">\n",
    "<br>\n",
    "Like it says, this table outlines the number of domestic and international flights for a particular month in a year. \n",
    "Now, copying this data for this table into Excel might be easy, but what if you had to do this a 100 times? \n",
    "\n",
    "Before we even think of this, we need to get a way to get a page with a particular carrier (say \"American Airlines\" and a particular airport (say \"Jackson International Airport, Atlanta\") to scrape. If you notice carefully, then you would have seen that the URI of the page does not change despite selecting the data. This is a real problem!\n",
    "\n",
    "Thanks to Chrome's inspector element, there's no need to worry!\n",
    "Right click on the Inspector: Inspect > Network, like so: <br>\n",
    "<img src = \"Screenshot (110).png\" style = \"width: 60%; height: 60%\"><br><br>\n",
    "\n",
    "Double-click on the highlighted text. It will open a small window next to the text. Scroll down to form parameters: **this** is the data that is sent with each request! \n",
    "\n",
    "At the bottom, you will be able to see the airport and carrier you chose. These are the only parts of the form data that changes for this page. \n",
    "\n",
    "### Looking for these form parameters\n",
    "If you examine the HTML for this page, you will not be able to find these values readily. Time to dive into some Python coding to look for these. (These parameters are actually well hidden in divs, and thus are not found easily.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      "<head><title>\r\n",
      "\tData Elements\r\n",
      "</title><link href=\"styles/global.css\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"styles/rita_main.css\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://fonts.googleapis.com/css?family=Open+Sans\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"https://www.bts.dot.gov/sites/bts.dot.gov/themes/bts_standalone/bts_standalone.css\" rel=\"stylesheet\"/><link href=\"https://www.bts.dot.gov/sites/bts.dot.gov/themes/bts_standalone/bts_standalone_pn.css\" rel=\"stylesheet\"/>\n",
      "<script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.2.6/jquery.min.js\" type=\"text/javascript\"></script>\n",
      "<script src=\"https://www.bts.dot.gov/sites/bts.dot.gov/themes/bts_standalone/bts_standalone.js\"></script>\n",
      "<script language=\"javascript\" type=\"text/javascript\">\r\n",
      "function window_CarrierList(page)\r\n",
      "{\r\n",
      "    //aUrl=\"Carrie"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 1000 exceeded with 425589 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BeautifulSoup functions to return the HTML page\n",
    "r = s.get(scrape_page)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "print soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by the output above, we now have the HTML page output to perform our calculations! \n",
    "\n",
    "After having finally found these parameters in the Inspector, I noticed that each parameter could be identified by its *id* of the same name. \n",
    "\n",
    "Additionally, some of the parameters I inspected seemed to be empty. These will likely be empty throughout my analysis, so I don't need to worry about them. The rest of the story is a simple loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__EVENTVALIDATION': '/wEdANQJI1Sqg9z91K1IxN+WYuLY9Pc8W+KtN+Mo2aSUQqdKHnn1wNLGBL09odqqO9CBtbAJsDUC3lheYiZJgW/YlADjuWzIaw6NWZdPqqJzsUIui3WJba4zPjTfLRRsH0Y4tKCbvFwJUL16Fg2zvSQ8pNpmmiXKEOf5q1Kv3vsuvzhW05PQbvopHFZM7OfQDQb4hdOUdXAXRWTwuBeN66YRUYZW+iHUadUKYEzQDBGkXUs/YCGr0cwlD3oBni9ShkvD3kwjk8WcekoqTVcnps8CXSCz2VxSHFLZn8o/OI/mzSaJLF7n4FW7/iSCbjzg5qjsDMH5Z4x2xKMscyTvkWGm4eCnGGT+PhzP2oB89KGJMNTRcLt8dfZ0OLmTKBRvL+6aLO1Jlqb4uy82+C1G/TuY290BKE0bVp+gYhNwVZBEHAug4oRNRIquFUPBmZHKNQbPcLdxJCrNuditKsxyOfCvi1s9pWWvurw18YSXxClfb0sw/6b9lN2DZRhnQ66+hD+aycC8f25oq7hT+8oWmhbKNSmthde8aZvm7cigiu7lj5rG9vMpuFq433U3WFpHq4V3rSYKlbsk7FCG4pN34VJPbxjIui8imvQgdNZYuF9tBmSfCMGWAtxVxwaJCe8yj4uJmba+0Pm8y2EfbpmeJ+44i6d8EmJqbmGABn8t1Ju434eerRoBJeHv60FRHHjWan2JtV04PuDW5M7IdHpDmLWkPtRYohRxs++m7m25rVGGO9xU1ZNF9UtcLUUhOKfxV7PKBup85Idz8i2sTT70Zc4lsW/JRHHlTXbctzOEfZD1Wprgm7Ac5PMqE0ra5Hs9xfdZld/gpyDYaqFgSrclhTGylLvDltyRWe3GPJgdPBxKU3ZsOOZKxTIQ2tefAP/tG5+tF/1bZ2SCbsUSUtVIjP7WdSJnLcMBNra8/krTTsDAFbBxcTaM9rKNoB1wQsxdHU68DHLOnCOgqQrN4"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 1000 exceeded with 221293 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for parameter in [\"__VIEWSTATE\", \"__VIEWSTATEGENERATOR\", \"__EVENTVALIDATION\"]: # other parameters are empty\n",
    "    parameter_id = soup.find(id = parameter)\n",
    "    if parameter_id is None:\n",
    "        data[parameter] = \"\"\n",
    "    else:\n",
    "        data[parameter] = parameter_id.get(\"value\")\n",
    "\n",
    "print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the form data!\n",
    "The only thing we need now is the list of carriers and airports to iterate over.\n",
    "\n",
    "Below are some functions that operate similarly to how we gathered the form data. Each of the dropdown options is contained as children of a *select* tag with id = \"CarrierList\"/\"AirportList\".\n",
    "\n",
    "### Data Cleaning\n",
    "As part of the extraction, it is necessary to get the relevant data and remove all extraneous information. \n",
    "In the functions below, I clean the data in 2 ways:\n",
    "1. Extract the \"value\" attribute from the tags\n",
    "2. Check to see if only relevant \"values\" are extracted\n",
    "\n",
    "How this has been done programmatically can be seen in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_carriers(page):\n",
    "    '''\n",
    "    function to extract list of carriers (which are displayed as a drop down) from [page].\n",
    "    '''\n",
    "    data = []\n",
    "\n",
    "    r = s.get(page)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    carrier_list = soup.find(id = \"CarrierList\")\n",
    "    children = carrier_list.findChildren()\n",
    "    \n",
    "    for airlines in children:\n",
    "        # this is what is contained in airlines: <option value=\"AA\">American Airlines </option>\n",
    "        \n",
    "        value = airlines.get('value')\n",
    "        # this is what is contained in value: AA\n",
    "        \n",
    "        # making sure we don't extract options such as \"AllUS\" in <option value=\"AllUS\">All U.S. Carriers</option>\n",
    "        if (len(value) == 2):\n",
    "            data.append(value)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "carriers = extract_carriers(scrape_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_airports(page):\n",
    "    '''\n",
    "    function to extract list of airports (which are displayed as a drop down) from [page].\n",
    "    '''\n",
    "    data = []\n",
    "    r=s.get(page)\n",
    "    soup=BeautifulSoup(r.text, \"lxml\")\n",
    "    carrier_list = soup.find(id = \"AirportList\")\n",
    "    children = carrier_list.findChildren()\n",
    "    for airlines in children:\n",
    "        value = airlines.get('value')\n",
    "        if (len(value) == 3 and value.isupper()):\n",
    "            data.append(value)\n",
    "            print value\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL\n",
      "BWI\n",
      "BOS\n",
      "CLT\n",
      "MDW\n",
      "ORD\n",
      "DAL\n",
      "DFW\n",
      "DEN\n",
      "DTW\n",
      "FLL\n",
      "IAH\n",
      "LAS\n",
      "LAX\n",
      "MIA\n",
      "MSP\n",
      "JFK\n",
      "LGA\n",
      "EWR\n",
      "MCO\n",
      "PHL\n",
      "PHX\n",
      "PDX\n",
      "SLC\n",
      "SAN\n",
      "SFO\n",
      "SEA\n",
      "TPA\n",
      "DCA\n",
      "IAD\n",
      "UXM\n",
      "ABR\n",
      "ABI\n",
      "DYS\n",
      "ADK\n",
      "VZF\n",
      "BQN\n",
      "AKK\n",
      "KKI\n",
      "AKI\n",
      "AKO\n",
      "CAK\n",
      "7AK\n",
      "KQA\n",
      "AUK\n",
      "ALM\n",
      "ALS\n",
      "ABY\n",
      "ALB\n",
      "ABQ\n",
      "ZXB\n",
      "WKK\n",
      "AED\n",
      "AEX\n",
      "AXN\n",
      "AET\n",
      "ABE\n",
      "AIA\n",
      "APN\n",
      "DQH\n",
      "AOO\n",
      "AMA\n",
      "ABL\n",
      "OQZ\n",
      "AOS\n",
      "OTS\n",
      "AKP\n",
      "EDF\n",
      "DQL\n",
      "MRI\n",
      "ANC\n",
      "AND\n",
      "AGN\n",
      "ANI\n",
      "ANN\n",
      "ANB\n",
      "ANV\n",
      "ATW\n",
      "ACV\n",
      "ARC\n",
      "ADM\n",
      "AVL\n",
      "HTS\n",
      "ASE\n",
      "AST\n",
      "AHN\n",
      "AKB\n",
      "PDK\n",
      "FTY\n",
      "ACY\n",
      "ATT\n",
      "ATK\n",
      "MER\n",
      "AUO\n",
      "AGS\n",
      "AUG\n",
      "AUS\n",
      "A28\n",
      "BFL\n",
      "BGR\n",
      "BHB\n",
      "BRW\n",
      "BTI\n",
      "BQV\n",
      "A2K\n",
      "BTR\n",
      "BTL\n",
      "AK2\n",
      "A56\n",
      "BTY\n",
      "BPT\n",
      "BVD\n",
      "WBQ\n",
      "BKW\n",
      "BED\n",
      "A11\n",
      "KBE\n",
      "BLV\n",
      "BLI\n",
      "BLM\n",
      "JVL\n",
      "BVU\n",
      "BJI\n",
      "RDM\n",
      "BEH\n",
      "BET\n",
      "BTT\n",
      "BVY\n",
      "OQB\n",
      "A50\n",
      "BIC\n",
      "BIG\n",
      "BGQ\n",
      "BMX\n",
      "PWR\n",
      "A85\n",
      "BIL\n",
      "BIX\n",
      "BGM\n",
      "KBC\n",
      "BHM\n",
      "BIS\n",
      "BYW\n",
      "BID\n",
      "BMG\n",
      "BMI\n",
      "BFB\n",
      "BYH\n",
      "BCT\n",
      "BOI\n",
      "RLU\n",
      "BXS\n",
      "BLD\n",
      "BYA\n",
      "BWG\n",
      "BZN\n",
      "BFD\n",
      "A23\n",
      "BRD\n",
      "BKG\n",
      "BWC\n",
      "PWT\n",
      "KTS\n",
      "BDR\n",
      "TRI\n",
      "BKX\n",
      "RBH\n",
      "BRO\n",
      "BWD\n",
      "BQK\n",
      "BCE\n",
      "BKC\n",
      "BUF\n",
      "IFP\n",
      "BUR\n",
      "BRL\n",
      "BTV\n",
      "MVW\n",
      "BNO\n",
      "BTM\n",
      "USA\n",
      "UXI\n",
      "CDW\n",
      "C01\n",
      "ADW\n",
      "CDL\n",
      "CGI\n",
      "LUR\n",
      "EHM\n",
      "CZF\n",
      "A61\n",
      "A40\n",
      "CYT\n",
      "MDH\n",
      "CLD\n",
      "CNM\n",
      "A87\n",
      "CPR\n",
      "CDC\n",
      "CID\n",
      "JRV\n",
      "NRR\n",
      "CEM\n",
      "CDR\n",
      "CIK\n",
      "CMI\n",
      "WCR\n",
      "CHS\n",
      "CRW\n",
      "SPB\n",
      "STT\n",
      "CHO\n",
      "CYM\n",
      "CHA\n",
      "CYF\n",
      "WA7\n",
      "CEX\n",
      "EGA\n",
      "NCN\n",
      "KCN\n",
      "VAK\n",
      "CYS\n",
      "PWK\n",
      "DPA\n",
      "LOT\n",
      "CKX\n",
      "CIC\n",
      "CEF\n",
      "KCG\n",
      "KCL\n",
      "WQZ\n",
      "KCQ\n",
      "CZN\n",
      "CIV\n",
      "ZXH\n",
      "SSB\n",
      "STX\n",
      "CHU\n",
      "LUK\n",
      "CVG\n",
      "OQC\n",
      "A12\n",
      "CHP\n",
      "IRC\n",
      "CLP\n",
      "CKB\n",
      "BKL\n",
      "CLE\n",
      "CGF\n",
      "CFT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 1000 exceeded with 4820 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airports = extract_airports(scrape_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than create folders for each of the airports and carriers for **testing**, I will only consider a small subset to see if my programs run correctly. To allow for the entire files, just uncomment the lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partial data to test\n",
    "\n",
    "airports_sample = sorted(airports[:3])\n",
    "# UNCOMMENT: airports_sample = sorted(airports) \n",
    "\n",
    "# print airports_sample\n",
    "\n",
    "carriers_sample = sorted(carriers[:3])\n",
    "# UNCOMMENT: carriers_sample = sorted(carriers) \n",
    "\n",
    "# print carriers_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizing the parameter extraction written way above, I have written the *extract_params* function to extract a set of given parameters for any page that you want to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_params(scrape_page, params):\n",
    "    '''\n",
    "    function to find certain parameters specified by [params] from [scrape_page], if they exist.\n",
    "    '''\n",
    "    \n",
    "    data = {}\n",
    "    r = s.get(scrape_page)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        \n",
    "    for parameter in params:\n",
    "        parameter_id = soup.find(id = parameter)\n",
    "        if parameter_id is None:\n",
    "            data[parameter] = \"\"\n",
    "        else:\n",
    "            data[parameter] = parameter_id.get(\"value\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *create_files* function below is the result of everything we wrote above. \n",
    "\n",
    "It takes in a set of parameters, lists of airports and carriers, to create a folder structure with relevant files inside each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_files(data, airports, carriers):\n",
    "    '''\n",
    "    function to make an HTTPS request to the server, given parameter values from [data] \n",
    "    for all values in [airports] and [carriers] and store the corresponding result in \n",
    "    a folder structure.\n",
    "    '''\n",
    "    \n",
    "    # go inside airport-data\n",
    "    os.chdir(\"airport-data\")\n",
    "        \n",
    "    # all data parameters have been stored in data\n",
    "    eventvalidation = data[\"__EVENTVALIDATION\"]\n",
    "    viewstate = data[\"__VIEWSTATE\"]\n",
    "    viewstategenerator = data[\"__VIEWSTATEGENERATOR\"]\n",
    "      \n",
    "    for airport in airports:\n",
    "        \n",
    "        # create new folder for each airport\n",
    "        newpath = airport \n",
    "\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "            \n",
    "        # print newpath\n",
    "\n",
    "        # navigate to the new folder\n",
    "        os.chdir(newpath)\n",
    "        \n",
    "        for carrier in carriers:\n",
    "            \n",
    "            # make a POST request for the current airport and carrier \n",
    "            r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                       data = ((\"__EVENTTARGET\", \"\"),\n",
    "                               (\"__EVENTARGUMENT\", \"\"),\n",
    "                               (\"__VIEWSTATE\", viewstate),\n",
    "                               (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                               (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                               (\"CarrierList\", carrier),\n",
    "                               (\"AirportList\", airport),\n",
    "                               (\"Submit\", \"Submit\")))\n",
    "            \n",
    "            # write new file\n",
    "            f = open(\"{0}-{1}.html\".format(airport, carrier), \"w\")\n",
    "            print \"Created file!\", \"{0}-{1}.html\".format(airport, carrier)\n",
    "            \n",
    "            f.write(r.text)\n",
    "            f.close()\n",
    "            \n",
    "        os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of parameters required to make request\n",
    "params = [\"__VIEWSTATE\", \"__VIEWSTATEGENERATOR\", \"__EVENTVALIDATION\"]\n",
    "\n",
    "# value of parameters\n",
    "params_dict = extract_params(scrape_page, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created file! ATL-AA.html\n",
      "Created file! ATL-AS.html\n",
      "Created file! ATL-G4.html\n",
      "Created file! BOS-AA.html\n",
      "Created file! BOS-AS.html\n",
      "Created file! BOS-G4.html\n",
      "Created file! BWI-AA.html\n",
      "Created file! BWI-AS.html\n",
      "Created file! BWI-G4.html\n"
     ]
    }
   ],
   "source": [
    "create_files(params_dict, airports_sample, carriers_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the structure of the system looks after executing the above function: <br><br>\n",
    "<img src = \"Screenshot (111).png\" style = \"width: 60%; height: 60%;\"><br><br>\n",
    "\n",
    "You can see that there is a folder for each airport and HTML files for each carrier within each folder, containing information regarding those particular airport and carrier that we can extract with our functions defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written below is a function that extracts the tabular data from any HTML page. Given the above folder structure, we can exploit the function and loop it to get data for all the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_table_data(page):\n",
    "    '''\n",
    "    function to extract all data present in the <table> element of [page] programmatically\n",
    "    and store this data.\n",
    "    \n",
    "    The resultant data structure is a list of dictionaries, where each dictionary is data \n",
    "    corresponding to a particular time frame.\n",
    "    '''\n",
    "    \n",
    "    html = urllib.urlopen(page).read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    table = soup.find('table', class_ = \"dataTDRight\")\n",
    "    tr =  table.find_all('tr', class_ = \"dataTDRight\")\n",
    "    for tags in tr:\n",
    "        inner_data = []\n",
    "        info = {}\n",
    "        td = tags.find_all('td')\n",
    "        for inner_tags in td:\n",
    "            inner_data.append(inner_tags.text)\n",
    "\n",
    "        if (inner_data[1] != \"TOTAL\"):\n",
    "            # notice the casting to an integer is not normal\n",
    "            info[\"year\"] = convert_to_int(inner_data[0]) \n",
    "            info[\"month\"] = convert_to_int(inner_data[1])\n",
    "            info[\"domestic\"] = convert_to_int(inner_data[2])\n",
    "            info[\"international\"] = convert_to_int(inner_data[3])\n",
    "            \n",
    "            # print info\n",
    "            data.append(info)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'domestic': 815489, 'international': 92565, 'month': 10, 'year': 2002},\n",
       " {'domestic': 766775, 'international': 91342, 'month': 11, 'year': 2002},\n",
       " {'domestic': 782175, 'international': 96881, 'month': 12, 'year': 2002},\n",
       " {'domestic': 785651, 'international': 98053, 'month': 1, 'year': 2003},\n",
       " {'domestic': 690750, 'international': 85965, 'month': 2, 'year': 2003},\n",
       " {'domestic': 797634, 'international': 97929, 'month': 3, 'year': 2003},\n",
       " {'domestic': 766639, 'international': 89398, 'month': 4, 'year': 2003},\n",
       " {'domestic': 789857, 'international': 87671, 'month': 5, 'year': 2003},\n",
       " {'domestic': 798841, 'international': 95435, 'month': 6, 'year': 2003},\n",
       " {'domestic': 832075, 'international': 102795, 'month': 7, 'year': 2003},\n",
       " {'domestic': 831185, 'international': 102145, 'month': 8, 'year': 2003},\n",
       " {'domestic': 782264, 'international': 90681, 'month': 9, 'year': 2003},\n",
       " {'domestic': 818777, 'international': 91820, 'month': 10, 'year': 2003},\n",
       " {'domestic': 766266, 'international': 91004,"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 1000 exceeded with 12963 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample output\n",
    "extract_table_data(scrape_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Notice that my program skips the line with \"TOTAL\" in one of the columns; this is because I want pure data, and no precomputed values for data consistency.\n",
    "\n",
    "You will also notice that I do not cast the values in the above function to integers directly. To see why, look at the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int(u'\\xa0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this, I have put the integer casting in a try block, and this returns 0 if the casting is unsuccessful.\n",
    "\n",
    "Now this is well and good, but you might be wondering how I noticed this error. Remember how I told you that always try your functions on a smaller dataset? Well, that's how I found that a value wasn't being cast properly and according wrote this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_int(val):\n",
    "    '''\n",
    "    helper function to try and convert a value to its corresponding integer value, else 0\n",
    "    '''\n",
    "    try:\n",
    "        int_val = int(val.replace(',', ''))\n",
    "        return int_val\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Converting the scraped data into a more accessible format\n",
    "Hurray! We know have the data we need; however, the data is present as a list of dictionaries.\n",
    "To perform statistical analyses or draw plots for the data, it would make sense to have this data in a more convenient format for Python's data anaylsis tools such as NumPy or Pandas. \n",
    "\n",
    "Luckily, dictionaries can be converted to Pandas DataFrames with the inbuilt *from_dict* Pandas function. I have wrapped this function in my own function for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_dataframe(dic):\n",
    "    '''\n",
    "    function to convert dictionary data [dic] into a pandas DataFrame\n",
    "    '''\n",
    "    return pd.DataFrame.from_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d is a dictionary\n",
    "d = extract_table_data(scrape_page)\n",
    "\n",
    "# df is a pandas dataframe\n",
    "df = convert_to_dataframe(d)\n",
    "\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Now that that we have *extract_table_data*, I have written a function generalizing this all files within the \"airport-data\" folder. \n",
    "\n",
    "This gives rise to a list, where each element concerns a particular airport. Within each \"airport\" element, there is a list of dictionaries corresponding to a different carrier from that airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_data_list():\n",
    "    \n",
    "    g_list = []\n",
    "    os.chdir(\"C:\\Users\\Raghav\\Desktop\\Data Analysis Nanodegree\\Data Wrangling\")\n",
    "    os.chdir(\"airport-data\")\n",
    "\n",
    "    # print os.listdir(os.getcwd())\n",
    "\n",
    "    for dir in os.listdir(os.getcwd()):\n",
    "        # go inside\n",
    "        os.chdir(dir)\n",
    "\n",
    "        # print os.listdir(os.getcwd())\n",
    "\n",
    "        l_list = []\n",
    "\n",
    "        for inner_dir in os.listdir(os.getcwd()):\n",
    "\n",
    "            l_dic = {}        \n",
    "            \n",
    "            print \"Reading file:\", inner_dir\n",
    "            \n",
    "             # print inner_dir\n",
    "            try:\n",
    "                \n",
    "                dic = extract_table_data(inner_dir)\n",
    "                print \"Converted {0}!\".format(inner_dir)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                dic = {}\n",
    "                print \"Could not convert {0} \".format(inner_dir)\n",
    "\n",
    "            df = convert_to_dataframe(dic)\n",
    "\n",
    "            inner_name = inner_dir.split(\".\")[0]\n",
    "            l_dic[inner_name] = df\n",
    "\n",
    "            l_list.append(l_dic)\n",
    "            print \"\"\n",
    "\n",
    "        g_list.append(l_list)\n",
    "\n",
    "        # come out\n",
    "        os.chdir(\"..\")\n",
    "    return g_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "airport_sample_data = convert_to_data_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all this data, we would need a good interface for any user to interact with this data. To that end, the function below, *get_data* does exactly that! \n",
    "\n",
    "As the user, you just need to provide the airport and carrier whose information you seek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(airport, carrier):\n",
    "    ind_airport = airports_sample.index(airport) # change this line later\n",
    "    ind_carrier = carriers_sample.index(carrier) # change this too\n",
    "    text = str(airport) + \"-\" + str(carrier)\n",
    "    print ind_airport, ind_carrier, text\n",
    "    return airport_sample_data[ind_airport][ind_carrier][text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example usage: airport is ATL and carrier is AA\n",
    "atl_aa = get_data('ATL', 'AA')\n",
    "atl_aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analysing the extracted data\n",
    "Phew! That was a lot!\n",
    "\n",
    "Now that we have the data we always wanted in a convenient DataFrame, let's see if we can spot some trends in the data.\n",
    "\n",
    "Consider what we have been working with: the ATL airport and AA airlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions to extract certain details \n",
    "# example in next cell\n",
    "\n",
    "def get_year(df, yr):\n",
    "    return df[df['year'] == yr]\n",
    "\n",
    "def get_month(df, month):\n",
    "    return df[df['month'] == month]\n",
    "\n",
    "def get_column_value(df, column, value):\n",
    "    return df[df[column] == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print all entries pertaining to the year of 2015\n",
    "atl_aa = get_data('ATL', 'AA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atl_aa_2015 = get_year(atl_aa, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting extracted data\n",
    "sns.barplot(data = atl_aa_2015, x = \"month\", y = \"domestic\")\n",
    "\n",
    "plt.xlabel('Month number')\n",
    "plt.ylabel('Number of domestic flights')\n",
    "plt.title('Domestic AA flights from ATL for 2015')\n",
    "plt.ylim(atl_aa_2015['domestic'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph above, the jump from June to July does seem like a lot for AA in 2015! \n",
    "\n",
    "After Googling around a bit, I found  an <a href = \"http://www.atl.com/wp-content/uploads/2017/01/07-01-2015ATL-to-exceed-national-travel-projections.pdf\">article</a>, which states how the airport has picked up immense traffic in July for the Independence Day weekend, almost 14% more passengers! American Airlines, being the <a href = \"https://en.wikipedia.org/wiki/List_of_largest_airlines_in_North_America\">largest airlines</a>, would thus see a higher increase.\n",
    "\n",
    "That was a sweet prediction! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "What did we cover in this project? \n",
    "\n",
    "We performed analysis on data sitting on web pages through sophisticated use of Python's data science libraries and extensive use of functions.\n",
    "\n",
    "Overall, this project serves as a pretty good overview of data wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to look at data about certain airlines and/or carriers, just make sure **airlines_sample** and **carriers_sample** contain those values, and rereun all the cells in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_data_list_from_carrier(carrier):\n",
    "    '''\n",
    "    function generalizing [extract_table_data] function for all files within \"airport-data\" folder \n",
    "    for a PARTICULAR carrier\n",
    "    '''\n",
    "\n",
    "    g_list = []\n",
    "    os.chdir(\"C:\\Users\\Raghav\\Desktop\\Data Analysis Nanodegree\\Data Wrangling\")\n",
    "    os.chdir(\"airport-data\")\n",
    "\n",
    "    for dir in os.listdir(os.getcwd()):\n",
    "        # go inside\n",
    "        os.chdir(dir)\n",
    "\n",
    "        for inner_dir in os.listdir(os.getcwd()):\n",
    "            \n",
    "            # break \"ATL-AA.html\" to \"ATL-AA\"\n",
    "            name = inner_dir.split(\".\")[0]\n",
    "            \n",
    "            # break \"ATL-AA\" to \"AA\"\n",
    "            carrier_name = name.split(\"-\")[1]\n",
    "            \n",
    "            if (carrier_name == carrier):\n",
    "            \n",
    "                l_dic = {}        \n",
    "\n",
    "                try:\n",
    "                    print \"Reading file:\", inner_dir\n",
    "                    dic = extract_table_data(inner_dir)\n",
    "\n",
    "                except:\n",
    "                    print \"Could not convert {0} \".format(inner_dir)\n",
    "                    dic = {}\n",
    "\n",
    "                df = convert_to_dataframe(dic)\n",
    "\n",
    "                inner_name = inner_dir.split(\".\")[0]\n",
    "                print \"Converted {0}!\".format(inner_dir)\n",
    "                l_dic[inner_name] = df\n",
    "\n",
    "                g_list.append(l_dic)\n",
    "\n",
    "        # come out\n",
    "        os.chdir(\"..\")\n",
    "                \n",
    "    return g_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# References\n",
    "1. <a href = \"https://in.udacity.com/course/data-analyst-nanodegree--nd002/\">Udacity Nanodegree (Data Science)</a>\n",
    "2. <a href = \"http://www.atl.com/wp-content/uploads/2017/01/07-01-2015ATL-to-exceed-national-travel-projections.pdf\">ATL travel predictions</a>\n",
    "3. <a href = \"https://en.wikipedia.org/wiki/List_of_largest_airlines_in_North_America\"> List of largest airlines in North America (Wikipedia)</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
